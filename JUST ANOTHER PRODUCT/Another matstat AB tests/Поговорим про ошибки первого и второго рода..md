Мы знаем, что у нас есть два типа ошибок. Это ошибка первого и второго рода. Мы уже рассматривали это в разделе ml [[Метрики ( supervised  learning)]] 

Дак вот, `ошибка первого рода` - это `False Positive`. То есть мы отклоняем значение, которое на самом деле верно. 

`Ошибка второго рода` - это `True negative`. То есть мы принимаем значение как действительное, но при этом оно не верно.  

В рамках тестирование гипотез : `Ошибка первого рода` происходит, когда мы отвергаем нулевую гипотезу, хотя она на самом деле верна. Это ложное положительное заключение. `Ошибка второго рода` происходит, когда мы не отвергаем нулевую гипотезу, хотя альтернативная гипотеза на самом деле верна. Это ложное отрицательное заключение.

Очевидно, что уровень значимости ( α ) напрямую влияет на ошибку первого и второго рода. 

Вспомним, что такое уровень значимости. Уровень значимости _α_ — это пороговое значение, которое мы устанавливаем перед началом теста, и которое определяет, насколько необычными должны быть данные, чтобы отвергнуть _H0._ Если вероятность получить наблюдаемые данные, предполагая, что _H0_ верна, меньше _α_ , то мы отвергаем H0. По дефолту _α_ устанавливается на уровне 0.05. Крч, мы говорим, что доверительный интервал у нас 95% => что уровень значимости 5% то есть 0.05.

Получается так, что если уровень значимости (α) установлен на 0.05, это означает, что есть 5% шанс ошибочно отвергнуть нулевую гипотезу, когда она верна.

Давайте посмотрим на пример : 

![[Screenshot 2024-07-21 at 15.13.57.png]]

Например, у нас есть две гипотезы : 

${H_0}$ : p = ${p_0}$
${H_1}$ : p = ${p_a}$

Где ${p_a}$ > ${p_0}$

Зеленый у нас отображен тот случай, когда мы будет реджектить нулевую гипотезу. Давайте мы возьмем 95% доверительный интервал.  Синие распределение это наблюдаемое распределение, которое мы получили а красное это то распределение, которое будет в случае если p = ${p_a}$

Итак, например у нас так получилось что наше значение статистики правее нашей границы, то есть она будет лежать в области, показанной синим цветом. В таком случае, мы должны отвергнуть нулевую гипотезу в пользу альтернативной. Однако , так как она лежит в левой области, она все еще с определенной вероятностью может принадлежать распределению, где  p = ${p_0}$ то есть это наше синие распределение. В таком случае как раз мы будем отвергать нулевую гипотезу в пользу альтернативной, но при этом нулевая гипотеза верна. Это и будет ошибка первого рода  ( α )  и равна она в нашем 0.05. 

И наоборот, если мы например получили значение, которое находится левее нашей границы, то в таком случае мы не будем реджектить нулевую гипотезу, а наоборот останемся в ней и не уйдем в пользу альтернативной гипотезы. В таком случае, наш результат будет лежать в красной области. И мы видим что с определенной вероятностью, получится так, что мы не отвергаем нулевую гипотезу, при этом мы все еще можем принадлежать тому случае, где  p = ${p_a}$ ,  то есть распределению красного цвета. То есть мы не отвергаем нулевую гипотезу, хотя альтернативная гипотеза на самом деле верна. Это и есть ошибка второго рода. Она обозначается как ( ${\beta}$ ). 

Таким образом, как наш доверительный интервал влияет на наши ошибки. Если мы увеличиваем наш доверительный интервал, то есть сдвигаем нашу границу вправа, то тогда мы уменьшаем ошибку первого рода, но при этом увеличиваем ошибку второго рода. И наоборот. Если мы уменьшаем наш доверительный интервал, то есть сдвигаем его влево, то в таком случае мы увеличиваем ошибку первого рода, но при этом уменьшаем ошибку второго рода. То есть тут у нас получается такой трейд-офф. 

Мы знаем, что по сути у нас работает принцип презумпции нулевой гипотезы. То есть мы верим в нулевую гипотезу , пока данные не опровергнут ее. Именно поэтому , ошибки первого и второго рода неравнозначны. Перед экспериментом мы фиксируем  α  а уже  ${\beta}$  мы минимизируем по остаточному принципу. 





