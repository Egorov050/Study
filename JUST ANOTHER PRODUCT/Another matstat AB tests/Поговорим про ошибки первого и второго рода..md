Мы знаем, что у нас есть два типа ошибок. Это ошибка первого и второго рода. Мы уже рассматривали это в разделе ml [[Метрики ( supervised  learning)]] 

Дак вот, `ошибка первого рода` - это `False Positive`. То есть мы отклоняем значение, которое на самом деле верно. 

`Ошибка второго рода` - это `True negative`. То есть мы принимаем значение как действительное, но при этом оно не верно.  

В рамках тестирование гипотез : `Ошибка первого рода` происходит, когда мы отвергаем нулевую гипотезу, хотя она на самом деле верна. Это ложное положительное заключение. `Ошибка второго рода` происходит, когда мы не отвергаем нулевую гипотезу, хотя альтернативная гипотеза на самом деле верна. Это ложное отрицательное заключение.

Очевидно, что уровень значимости ( α ) напрямую влияет на ошибку первого и второго рода. 