Мы знаем, что у нас есть два типа ошибок. Это ошибка первого и второго рода. Мы уже рассматривали это в разделе ml [[Метрики ( supervised  learning)]] 

Дак вот, `ошибка первого рода` - это `False Positive`. То есть мы отклоняем значение, которое на самом деле верно. 

`Ошибка второго рода` - это `True negative`. То есть мы принимаем значение как действительное, но при этом оно не верно.  

В рамках тестирование гипотез : Ошибка первого рода происходит, когда мы отвергаем нулевую гипотезу, хотя она на самом деле верна. Это ложное положительное заключение.