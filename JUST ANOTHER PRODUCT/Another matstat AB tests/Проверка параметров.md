Для того, чтобы понять, правильные ли мы параметры выбрали, мы должны сравнить выдает ли наш тест такие же ошибки первого и второго рода, какие мы и установили ранее. Для этого, проведем сначала `АА тест`

`АА тест (AA-тестирование)` используется для проверки и подтверждения корректности методологии АБ тестирования (A/B-тестирования). Он помогает убедиться, что система АБ тестирования работает правильно и не имеет встроенных ошибок, которые могли бы привести к некорректным результатам. `Основная цель АА теста — убедиться, что наша система A/B тестирования не создает ложных различий. Если система приводит к частым ложным различиям, это указывает на высокую вероятность ошибки первого рода, и значит, мы не можем доверять результатам будущих A/B тестов.` Вспомним еще раз про ошибку первого рода :  `тест показывает наличие разницы между вариантами, хотя на самом деле никакой разницы нет`.  Мы это и можем проверить при помощи АА теста.

Приведем самый простой пример, как мы можем провести АА тест.
Для начала определимся с нашими параметрами и поймем размер выборки : 

```python

alpha = 0.05                    # вероятность ошибки I рода
beta = 0.2                      # вероятность ошибки II рода
mu_control = 2500               # средняя выручка с пользователя в контрольной группе
effect = 100                    # размер эффекта
mu_pilot = mu_control + effect  # средняя выручка с пользователя в экспериментальной группе
std = 800                       

t_alpha = stats.norm.ppf(1 - alpha / 2, loc=0, scale=1)
t_beta = stats.norm.ppf(1 - beta, loc=0, scale=1)
var = 2 * std ** 2
sample_size = int((t_alpha + t_beta) ** 2 * var / (effect ** 2))
print(f'sample_size = {sample_size}')

```

Отлично, теперь мы можем перейти к выловке ошибок первого рода : 

```python
type_one_error = []



for i in range(1000):
    data1 = np.random.normal(mu_control, std, size = sample_size)
    data2 = np.random.normal(mu_control, std, size = sample_size)

    _, p_value_aa = stats.ttest_ind(data1, data2)
    type_one_error.append(p_value_aa < alpha)
    
```

Проверим, сколько какое значение у нас получилось : 

```python
print(np.mean(type_one_error)) 

0.046
```

Отлично, так как у нас ошибка получилась такая же , какую мы и предполагали, то за ошибку первого рода мы можем не волноваться. 

Перейдем к проверке ошибки второго рода. 
Вспомним, что основная идея ошибки второго рода заключается в том, что наш эксперимент не показывает различие там, где она есть. То есть проще говоря не видит различий между выборками, а оно на самом деле есть. 

Для того, чтобы поймать эту ошибку, представим, что мы уже получили новые данные, где среднее новых данных равно : 

```python
effect = 100                    # размер эффекта
mu_pilot = mu_control + effect  # средняя выручка с пользователя в экспериментальной группе

```

наш размер выборки будет точно такой же, что и сверху. 
итак, перейдем к генерации нового распределения и вывловке ошибки второго рода : 

```python
type_two_error = []



for i in range(1000):
    data1 = np.random.normal(mu_control, std, size = sample_size)
    pivot = np.random.normal(mu_pilot, std, size = sample_size)

    _, p_value_ab = stats.ttest_ind(pivot, data1)
    type_one_error.append(p_value_ab > alpha)

print(np.mean(type_two_error))

0.215

```

Мы получили приблизительно такое же значение, какое и хотели получить. Значит у нас все хорошо

