Для того, чтобы понять, правильные ли мы параметры выбрали, мы должны сравнить выдает ли наш тест такие же ошибки первого и второго рода, какие мы и установили ранее. Для этого, проведем сначала `АА тест`

`АА тест (AA-тестирование)` используется для проверки и подтверждения корректности методологии АБ тестирования (A/B-тестирования). Он помогает убедиться, что система АБ тестирования работает правильно и не имеет встроенных ошибок, которые могли бы привести к некорректным результатам. `Основная цель АА теста — убедиться, что наша система A/B тестирования не создает ложных различий. Если система приводит к частым ложным различиям, это указывает на высокую вероятность ошибки первого рода, и значит, мы не можем доверять результатам будущих A/B тестов.` Вспомним еще раз про ошибку первого рода :  `тест показывает наличие разницы между вариантами, хотя на самом деле никакой разницы нет`.  Мы это и можем проверить при помощи АА теста.

Приведем самый простой пример, как мы можем провести АА тест.
Для начала определимся с нашими параметрами и поймем размер выборки : 

```python

alpha = 0.05                    # вероятность ошибки I рода
beta = 0.2                      # вероятность ошибки II рода
mu_control = 2500               # средняя выручка с пользователя в контрольной группе
effect = 100                    # размер эффекта
mu_pilot = mu_control + effect  # средняя выручка с пользователя в экспериментальной группе
std = 800                       

t_alpha = stats.norm.ppf(1 - alpha / 2, loc=0, scale=1)
t_beta = stats.norm.ppf(1 - beta, loc=0, scale=1)
var = 2 * std ** 2
sample_size = int((t_alpha + t_beta) ** 2 * var / (effect ** 2))
print(f'sample_size = {sample_size}')

```

Отлично, теперь мы можем перейти к выловке ошибок первого рода : 

```python
type_one_error = []



for i in range(1000):
    data1 = np.random.normal(mu_control, std, size = sample_size)
    data2 = np.random.normal(mu_control, std, size = sample_size)

    _, p_value_aa = stats.ttest_ind(data1, data2)
    type_one_error.append(p_value_aa < alpha)
    
```

Проверим, сколько какое значение у нас получилось : 

```python
print(np.mean(type_one_error)) 

0.046
```

Отлично, так как у нас ошибка получилась такая же , какую мы и предполагали, то за ошибку первого рода мы можем не волноваться. 

Перейдем к проверке ошибки второго рода. 
Вспомним, что основная идея ошибки второго рода заключается в том, что наш эксперимент не показывает различие там, где она есть. То есть проще говоря не видит различий между выборками, а оно на самом деле есть. 

Для того, чтобы поймать эту ошибку, представим, что мы уже получили новые данные, где среднее новых данных равно : 

```python
effect = 100                    # размер эффекта
mu_pilot = mu_control + effect  # средняя выручка с пользователя в экспериментальной группе

```

наш размер выборки будет точно такой же, что и сверху. 
итак, перейдем к генерации нового распределения и вывловке ошибки второго рода : 

```python
type_two_error = []



for i in range(1000):
    data1 = np.random.normal(mu_control, std, size = sample_size)
    pivot = np.random.normal(mu_pilot, std, size = sample_size)

    _, p_value_ab = stats.ttest_ind(pivot, data1)
    type_one_error.append(p_value_ab > alpha)

print(np.mean(type_two_error))

0.215

```

Мы получили приблизительно такое же значение, какое и хотели получить. Значит у нас все хорошо :)

Также , затем мы должны проверить распределение `p-value` как на АА , так и на АБ тесте. 

Для примера также сгенерируем две выборки и построим распределение. 

```python

# генерируем выборки

resultAA = []
resultAB = []
for i in range(1000):
    test = np.random.normal(100, 45, 1000)
    test2 = np.random.normal(100, 45, 1000)
    control = np.random.normal(105, 40, 1000)

    _, p = stats.ttest_ind(test, test2)
    resultAA.append(p)

    _, p = stats.ttest_ind(test, control)
    resultAB.append(p)

# строим CDF 

aa_sorted_pvalues = np.sort(resultAA)
aa_cdf = np.arange(1, len(aa_sorted_pvalues) + 1) / len(aa_sorted_pvalues)
plt.plot(aa_sorted_pvalues, aa_cdf, label='A/A', color='orange')

ab = np.sort(resultB)
ab_cdf = np.arange(1, len(resultAB) + 1) / len(ab)
plt.plot(ab , ab_cdf, label='A/B')

plt.legend(fontsize=12)
plt.xlabel('p-value', size=12)
plt.grid()
```

Получаем следующую картину : 

<img width="512" alt="Screenshot 2024-08-09 at 17 20 51" src="https://github.com/user-attachments/assets/556631f6-6af9-4617-8a17-1cbe17852591">

Рассмотрим распределение `p-value` для АБ теста. Лучшее возможное распределение для p-value это приближение к зеркальной букве Г ( Такая же ситуация как и с ROC - кривой ). Когда в группе B есть реальный эффект (например, среднее значение существенно отличается от группы A), статистический тест чаще будет отклонять нулевую гипотезу. В таких случаях p-value будет близко к 0, поскольку это указывает на низкую вероятность того , что наблюдаемая разница случайна. 


Почитать : 
https://habr.com/ru/companies/X5Tech/articles/596279/

