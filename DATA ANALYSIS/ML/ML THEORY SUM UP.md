Итак, у нас существует несколько видов машинного обучения : это supervised learning и unsupervised learning. Более подробно об этом описано здесь : [[Types of ML]]

Далее, если речь идет про классификацию или регрессию, то : 

Итак, перед тем как мы начнем обучать нашу модель, в первую очередь, мы должны подготовить данные. Для этого, мы по классике проверим на not null и так далее. 

Далее, мы должны проверить данные на наличие выбросов. Мы это можем проверить либо построив распределение ( hist ) для каждой features или применить local outlier factor. Подробнее о нем написано здесь :  [[Удаление выбросов ( LocalOutlierFactor )]] 

Также не забываем про обработку категориальных данных. [[Обработка категориальных данных]]

Далее рассмотрим supervised learning

Сначала поделить данные на features и target variables. Об этом написано здесь : [[Сплит данных перед моделью]]

Далее, мы должны поделить наши данные на training и test set. Об этом написано тоже здесь :  [[Сплит данных перед моделью]]. Очень важно не забыть про параметр stratify , который позволяет в тренировочном и тестовом семпле сохранять такую же пропорцию target variables , как и в изначальном датасете. Об этом тоже написано  [[Сплит данных перед моделью]]  

Далее, мы можем применить PCA, об этом также написано [[Сплит данных перед моделью]]. Стоит отметить, что мы применяем PCA как к train set, так и для test set. 

Перейдем к масштабированию ( Scailing ). 
Мы его `не должны применять` к следующим алгоритмам : 
- Desicion Tree
- Random Forest
- Gradient boosting 

Для алгоритмов, таких как Decision Tree, Random Forest и Gradient Boosting, применение масштабирования (scaling) данных не имеет смысла по следующим причинам:
<h6>Решающее дерево (Decision Tree)</h6>
Работа с разделением данных : Решающие деревья строятся путем разделения данных на основе определенных пороговых значений признаков. Эти пороговые значения выбираются на основании информационных критериев, таких как индекс Джини или энтропия. Поскольку эти пороги зависят от относительного порядка значений признаков, а не от их абсолютных значений, масштабирование не влияет на процесс разделения.
<h6>Случайный лес (Random Forest)</h6>
Комбинация решающих деревьев : Случайный лес состоит из множества решающих деревьев, каждое из которых обучается на случайной подвыборке данных. Поскольку каждое отдельное дерево не требует масштабирования данных, то и объединение их в случайный лес не требует масштабирования.
<h6>Градиентный бустинг (Gradient Boosting) :</h6>
Построение последовательных деревьев : Градиентный бустинг строит модели путем последовательного добавления деревьев, где каждая новая модель корректирует ошибки предыдущих. Так как каждый этап включает в себя построение решающего дерева, которое не чувствительно к масштабированию, то и весь процесс градиентного бустинга не требует масштабирования данных.

<h3>Дополнительные детали:</h3>
- **Масштабирование и его цель**: Масштабирование применяется для алгоритмов, чувствительных к масштабам признаков, таких как линейные модели (линейная регрессия, логистическая регрессия) и модели, основанные на расстояниях (k-ближайших соседей, SVM). Цель масштабирования в этих случаях — избежать доминирования признаков с большими значениями над признаками с малыми значениями.
- **Стабильность модели**: Для деревьев и их ансамблей важна структура данных, а не их масштаб. Масштабирование не изменяет структуру данных и не оказывает значимого влияния на процесс построения моделей.

Таким образом, масштабирование признаков не улучшает и не изменяет производительность и точность моделей на основе деревьев решений, случайного леса или градиентного бустинга, что делает эту процедуру излишней для данных алгоритмов.

Соответсвенно, мы применяем масштабирование, только в том случае, если речь идет об линейных моделях. `Но не всегда масштабирование это хорошо`. Это мы можем посмотреть на примере данного кейса : https://github.com/Egorov050/scaling-is-not-always-good

Как только мы сделали мы можем перейти к тренировке наших алгоритмов. 











[[Подготовка данных]]
[[Сплит данных перед моделью]]
[[Модель ( KNN )  классификация и регрессия]]
[[Модель ( Gradient boosting )]]
[[Модель ( Random Forest )]]
[[Модель ( Decision Tree )]]
[[Модель ( Logistic Regression )]]
[[Обработка категориальных данных]]
