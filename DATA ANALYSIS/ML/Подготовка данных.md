Перед тем, как применять различные ml подели да и вообще работать с данными, нам нужно привести их в нормальное состояние. Рассмотрим несколько шагов.

<h2>Очистка данных</h2>
Удаление пропущенных значений:
-  *Удаление строк или столбцов* с пропущенными значениями.
- *Замена пропущенных значений* на средние, медианные или наиболее часто встречающиеся значения, или использование более сложных методов, таких как иммпутация. 
- *удаление дубликатов*

<h2>Обработка категориальных данных </h2>
[[Обработка категориальных данных]]

<h2>Масштабирование признаков</h2> 
Масштабирование признаков заключается в преобразовании данных таким образом, чтобы они имели схожие масштабы.

Масштабирование признаков необходимо для некоторых моделей машинного обучения, чтобы улучшить их производительность, численную стабильность и скорость сходимости. Ниже приведены категории моделей, которые требуют или не требуют масштабирования признаков.


<h3>Модели, которые нужно масштабировать признаки : </h3>
<h6>Методы на основе расстояния :</h6>
1) <span style="color:rgb(253, 165, 15)">K-Nearest Neighbors (KNN)</span>: Использует евклидово расстояние для определения ближайших соседей. Несмасштабированные признаки могут значительно исказить результаты.
2) <span style="color:rgb(253, 165, 15)">Кластеризация (например, K-Means)</span>: Расстояния между центроидами и точками данных зависят от масштаба признаков.
<h6>Линейные модели :</h6>
1) Логистическая регрессия и линейная регрессия: Масштабирование улучшает численную стабильность и скорость сходимости оптимизационных алгоритмов.


<h3>Модели, не требующие масштабирования</h3>
<h6>Деревья решений : </h6>
1) <span style="color:rgb(253, 165, 15)">Decision Trees</span>: Делают разбиения на основе порогов, поэтому масштаб данных не влияет на процесс разбиения.
2) <span style="color:rgb(253, 165, 15)">Random Forest</span>: Множество деревьев решений, не чувствительных к масштабу признаков.
3) <span style="color:rgb(253, 165, 15)">Gradient Boosting</span>: Так же как и деревья решений, не чувствителен к масштабу признаков.


<h2>Как мы это делаем?</h2>

```python 
from sklearn.preprocessing import StandardScaler
# Инициализация 
scaler = StandardScaler() 
# Обучение scaler на тренировочных данных 
scaler.fit(X_train) 
# Преобразование тренировочных данных 
X_train_normed = scaler.transform(X_train) 
# Преобразование тестовых данных 
X_test_normed = scaler.transform(X_test)
```

*Мы должны это делать уже после того, как почистили датасет и поделили на тренировочный и тестовый семпл ! 

<h4>Применяем мы к следующим типам данных :</h4> 
<span style="color:rgb(253, 165, 15)">Числовые (не категориальные) данные : </span>

1) Масштабирование применяется к числовым признакам, таким как возраст, зарплата, расстояние, количество и т.д.

2) Категориальные данные, которые были закодированы методом One-Hot Encoding, обычно не требуют масштабирования, так как они представлены в виде 0 и 1.

<span style="color:rgb(253, 165, 15)">Признаки с различными диапазонами значений :</span>

1) Если признаки имеют различные диапазоны значений, это может негативно повлиять на производительность модели. Например, если один признак варьируется от 1 до 10, а другой от 1000 до 10000, алгоритмы могут быть чувствительны к таким различиям.


*Примечание : Масштабирование числовых признаков после One-Hot Encoding не влияет на закодированные категориальные признаки. При масштабировании данные, полученные после one-hot encoding (столбцы с 0 и 1), останутся в неизменном виде, поскольку масштабирование не изменяет значения 0 и 1. Оно лишь приводит числовые признаки к заданному диапазону.*

Про масштабирование подробно здесь : https://www.youtube.com/watch?v=XsuCOfpf8Ic



