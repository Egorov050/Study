Деревья решений — это популярный метод машинного обучения для классификации и регрессии, который представляет собой модель в виде дерева, где каждая внутренняя вершина (или узел) представляет собой проверку одного из признаков, каждая ветвь — результат этой проверки, а каждое листовое узловое значение — прогнозируемый результат (класс или значение).

<h6>Как работают решающие деревья</h6>
Структура дерева:
- Корневой узел : Начальный узел дерева, содержащий все данные.
- Внутренние узлы : Узлы, в которых происходит проверка одного из признаков.
- Листовые узлы : Конечные узлы, которые дают окончательный результат (класс или значение).

Построение дерева: 
- Выбор признака и разбиение : На каждом внутреннем узле выбирается признак и порог, по которым данные делятся на подгруппы. Цель — максимизировать различие между подгруппами по какому-либо критерию (например, уменьшение неопределенности).

Прогнозирование:
- Для нового примера дерево проходит по узлам, проверяя значения признаков и перемещаясь по соответствующим веткам до достижения листового узла, который дает прогноз.
<h6>Как происходит деление? </h6>
`Выбор признака :` На каждом узле дерева выбирается признак, который лучше всего разделяет данные. Это делается с помощью критерия, такого как индекс Джини, энтропия или среднеквадратическая ошибка.

`Разделение по порогу :`  Для числовых признаков устанавливается порог, который делит данные на две группы. Для категориальных признаков данные разделяются по категориям.

`Оценка разбиения :` После разбиения вычисляется, насколько хорошо оно улучшает чистоту подгрупп (например, насколько они однородны по классам или значениям). Используются меры, такие как уменьшение неопределенности (энтропия) или индекс Джини.

`Принятие решения :`  Если выбранное разбиение приводит к значительному улучшению качества (например, высокая чистота групп), то разбиение выполняется. В противном случае, узел может стать листом или применяться другое разбиение.

<h6>Индекса Джини</h6>
Формула для подсчета индекса джини :

$$
Gini(R) = 1 - \sum_{i=1}^{k} p_i^2
$$

<h6>Пример</h6>

Допустим, в узле R присутствуют `три` класса `A`, `B` и `C` с долями pA=0.3, pB​=0.4, и pC​=0.3 соответственно. Тогда индекс Джини для узла R рассчитывается следующим образом:

$$
Gini(R) = 1 - (p_A^2 + p_B^2 + p_C^2)
        = 1 - (0.3^2 + 0.4^2 + 0.3^2)
        = 1 - (0.09 + 0.16 + 0.09)
        = 1 - 0.34
        = 0.66
$$
<h6>Интерпретация индекса Джини</h6>

`Высокий индекс Джини` : Значение индекса Джини близко к 0.5 указывает на высокую гетерогенность узла. Это означает, что классы распределены примерно поровну, и узел плохо разделяет классы. Такой узел менее информативен для построения дерева решений, так как он не способствует уменьшению неопределенности в данных.

`Низкий индекс Джини` : Значение индекса Джини близко к 0 указывает на низкую гетерогенность узла. Это означает, что один класс доминирует, и узел хорошо разделяет классы. Такой узел более информативен для построения дерева решений, так как он способствует значительному уменьшению неопределенности в данных.

Для `регрессионных задач` понятие неоднородности (heterogeneity) оценивается как дисперсия целевой переменной в выборке. Дисперсия представляет собой меру разброса или сходства данных в выборке.

Для задач `классификации` понятие неоднородности выборки связано с пропорциями классов в ней: если какой-то класс доминирует, то неоднородность будет низкой, а если распределение классов примерно равномерно, то неоднородность будет высокой.

Метрика прироста информации :

$$
Q(R) = H(R) - \frac{|R_l|}{|R|} H(R_l) - \frac{|R_r|}{|R|} H(R_r)
$$

**Прирост информации** (или прирост чистоты) Q(R), показывает, насколько разбиение улучшило однородность узлов, то есть насколько оно снизило их гетерогенность или неопределенность.

По факту, для того чтобы выбрать фичу, по которой мы будет проиходить дележка, мы высчитываем метрику прироста информации и выбираем наилучшую

<h2>Регрессия</h2>

```python 
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
```

Для начала испортируем все нужные нам библиотеки включая :

```python 
from sklearn.tree import DecisionTreeRegressor
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, обработали категориальные данные [[Обработка категориальных данных]],  приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Далее, приступаем к машинному обучению.

Сначала инициализируем модель : 

```python 
dt = DecisionTreeRegressor()
```

Затем тренируем нашу модель на `X_train, y_train` : 

```python 
dt.fit(X_train, y_train)
```

Модель натренирована. Теперь мы можем прогнать `X_test` через уже натренированую модель и получить `y_pred`, которую затем мы можем сравнить с `y_test` : 

```python 
y_pred = dt.predict(X_test)
```

<h2>Классификация</h2>
Для начала импортируем библиотеку : 

```python
from sklearn.tree import DecisionTreeClassifier
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Далее, приступаем к машинному обучению.

Сначала инициализируем модель : 

```python 
dt = DecisionTreeClassifier()
```

Затем тренируем нашу модель на `X_train, y_train` : 

```python 
dt.fit(X_train, y_train)
```

Модель натренирована. Теперь мы можем прогнать `X_test` через уже натренированую модель и получить `y_pred`, которую затем мы можем сравнить с `y_test` : 

```python 
y_pred = dt.predict(X_test)
```

<h2>Важные вещи</h2>

Для того, чтобы модель была неперетренирована, нам нужно указать максимально допустимую глубину дерева , делать мы можем это так : 

```python 
DTR = DecisionTreeRegressor(max_depth=5)
```

Это нужно, чтобы не было оверфиттинга

