Деревья решений — это популярный метод машинного обучения для классификации и регрессии, который представляет собой модель в виде дерева, где каждая внутренняя вершина (или узел) представляет собой проверку одного из признаков, каждая ветвь — результат этой проверки, а каждое листовое узловое значение — прогнозируемый результат (класс или значение).

<h6>Как работают решающие деревья</h6>
Структура дерева:
- Корневой узел : Начальный узел дерева, содержащий все данные.
- Внутренние узлы : Узлы, в которых происходит проверка одного из признаков.
- Листовые узлы : Конечные узлы, которые дают окончательный результат (класс или значение).

Построение дерева: 
- Выбор признака и разбиение : На каждом внутреннем узле выбирается признак и порог, по которым данные делятся на подгруппы. Цель — максимизировать различие между подгруппами по какому-либо критерию (например, уменьшение неопределенности).

Прогнозирование:
- Для нового примера дерево проходит по узлам, проверяя значения признаков и перемещаясь по соответствующим веткам до достижения листового узла, который дает прогноз.
<h6>Как происходит деление? </h6>
`Выбор признака :` На каждом узле дерева выбирается признак, который лучше всего разделяет данные. Это делается с помощью критерия, такого как индекс Джини, энтропия или среднеквадратическая ошибка.

`Разделение по порогу :`  Для числовых признаков устанавливается порог, который делит данные на две группы. Для категориальных признаков данные разделяются по категориям.

`Оценка разбиения :` После разбиения вычисляется, насколько хорошо оно улучшает чистоту подгрупп (например, насколько они однородны по классам или значениям). Используются меры, такие как уменьшение неопределенности (энтропия) или индекс Джини.

`Принятие решения :`  Если выбранное разбиение приводит к значительному улучшению качества (например, высокая чистота групп), то разбиение выполняется. В противном случае, узел может стать листом или применяться другое разбиение.

<h6>Индекса Джини</h6>
Формула для подсчета индекса джини :

$$
Gini(R) = 1 - \sum_{i=1}^{k} p_i^2
$$

<h6>Пример</h6>

Допустим, в узле R присутствуют `три` класса `A`, `B` и `C` с долями pA=0.3, pB​=0.4, и pC​=0.3 соответственно. Тогда индекс Джини для узла R рассчитывается следующим образом:

$$
Gini(R) = 1 - (p_A^2 + p_B^2 + p_C^2)
        = 1 - (0.3^2 + 0.4^2 + 0.3^2)
        = 1 - (0.09 + 0.16 + 0.09)
        = 1 - 0.34
        = 0.66
$$
<h6>Интерпретация индекса Джини</h6>

`Высокий индекс Джини` : Значение индекса Джини близко к 0.5 указывает на высокую гетерогенность узла. Это означает, что классы распределены примерно поровну, и узел плохо разделяет классы. Такой узел менее информативен для построения дерева решений, так как он не способствует уменьшению неопределенности в данных.

`Низкий индекс Джини` : Значение индекса Джини близко к 0 указывает на низкую гетерогенность узла. Это означает, что один класс доминирует, и узел хорошо разделяет классы. Такой узел более информативен для построения дерева решений, так как он способствует значительному уменьшению неопределенности в данных.

Для `регрессионных задач` понятие неоднородности (heterogeneity) оценивается как дисперсия целевой переменной в выборке. Дисперсия представляет собой меру разброса или сходства данных в выборке.

Для задач `классификации` понятие неоднородности выборки связано с пропорциями классов в ней: если какой-то класс доминирует, то неоднородность будет низкой, а если распределение классов примерно равномерно, то неоднородность будет высокой.

Метрика прироста информации :

$$
Q(R) = H(R) - \frac{|R_l|}{|R|} H(R_l) - \frac{|R_r|}{|R|} H(R_r)
$$

**Прирост информации** (или прирост чистоты) Q(R), показывает, насколько разбиение улучшило однородность узлов, то есть насколько оно снизило их гетерогенность или неопределенность.

Для выбора признака (фичи) и порога, по которому будет происходить разделение данных в дереве решений, используется метрика прироста информации. Сравниваем приросты информации для всех признаков и порогов и выбираем тот признак и порог, который дает наибольший прирост информации. 

<h2>Регрессия</h2>

```python 
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import <span style="color:rgb(0, 0, 0)"><span style="color:rgb(0, 0, 0)">DecisionTreeRegressor</span></span>
```

Для начала испортируем все нужные нам библиотеки включая :

```python 
from sklearn.tree import DecisionTreeRegressor
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, обработали категориальные данные [[Обработка категориальных данных]],  приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Далее, приступаем к машинному обучению.

Сначала инициализируем модель : 

```python 
dt = DecisionTreeRegressor()
```

Затем тренируем нашу модель на `X_train, y_train` : 

```python 
dt.fit(X_train, y_train)
```

Модель натренирована. Теперь мы можем прогнать `X_test` через уже натренированую модель и получить `y_pred`, которую затем мы можем сравнить с `y_test` : 

```python 
y_pred = dt.predict(X_test)
```

<h2>Классификация</h2>
Для начала импортируем библиотеку : 

```python
from sklearn.tree import DecisionTreeClassifier
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Далее, приступаем к машинному обучению.

Сначала инициализируем модель : 

```python 
dt = DecisionTreeClassifier()
```

Затем тренируем нашу модель на `X_train, y_train` : 

```python 
dt.fit(X_train, y_train)
```

Модель натренирована. Теперь мы можем прогнать `X_test` через уже натренированую модель и получить `y_pred`, которую затем мы можем сравнить с `y_test` : 

```python 
y_pred = dt.predict(X_test)
```

<h2>Гиперпараметры</h2>

Для того, чтобы модель была `неперетренирована`, нам нужно указать определенные `гиперпараметры`, а именно : 

`max_depth` : Максимальная глубина дерева. Ограничивает количество уровней в дереве. Это помогает предотвратить переобучение (overfitting) путем ограничения сложности модели. Меньшая глубина дерева может привести к недообучению (underfitting), в то время как большая глубина может вызвать переобучение.

`min_samples_split` : Минимальное количество образцов, необходимых для разделения узла. Определяет, сколько образцов должно быть в узле, чтобы выполнить разбиение. Значение по умолчанию зависит от реализации, но оно помогает контролировать переобучение, предотвращая разбиение узлов, содержащих слишком мало данных.

`min_samples_leaf` :  Минимальное количество образцов, которые должны быть в листовом узле. Определяет минимальное количество образцов, которые должны быть в узле, чтобы он был создан как конечный (листовой). Это предотвращает создание узлов, которые содержат очень мало данных, что также помогает в борьбе с переобучением.

`max_features` :  Максимальное количество признаков, которые будут рассматриваться при поиске лучшего разделения. Ограничивает количество признаков, используемых для разбиения узлов. Это может улучшить обобщающую способность модели, уменьшая корреляцию между деревьями в ансамблях, таких как случайные леса (Random Forest).

`max_leaf_nodes` :  Максимальное количество листовых узлов в дереве. Ограничивает количество конечных узлов (листов) в дереве. Это может помочь в предотвращении переобучения, обеспечивая, чтобы дерево не стало слишком сложным.

`min_impurity_decrease` :  Минимальное уменьшение нечистоты, необходимое для выполнения разбиения узла. Контролирует, насколько значительное улучшение чистоты требуется для разбиения узла. Это помогает управлять количеством разбиений и может предотвратить создание узлов с малым приростом информации.

`criterion` :  Функция для оценки качества разбиения. Определяет метрику, которая будет использоваться для оценки качества разделения. В задачах классификации обычно используются "gini" (Индекс Джини) или "entropy" (энтропия). Для регрессии обычно используются "mse" (среднеквадратичная ошибка) или "mae" (средняя абсолютная ошибка).

`splitter`Алгоритм, используемый для разбиения узлов. Определяет стратегию разбиения узлов. Значения могут быть "best" (выбирает лучшее разделение) или "random" (выбирает лучшее разделение случайным образом). "Best" обычно используется для достижения наилучших результатов, в то время как "random" может быть полезен для улучшения обобщающей способности.

Например, это может сделано быть следующим образом : 

```python 
from sklearn.tree import DecisionTreeClassifier

# Создание дерева решений с настройкой гиперпараметров
clf = DecisionTreeClassifier(
    criterion='gini',              # Или 'entropy'
    splitter='best',               # Или 'random'
    max_depth=10,                  # Максимальная глубина дерева
    min_samples_split=2,           # Минимальное количество образцов для разделения узла
    min_samples_leaf=1,            # Минимальное количество образцов в листовом узле
    max_features=None,             # Или количество признаков для рассмотрения
    max_leaf_nodes=None,           # Максимальное количество листов
    min_impurity_decrease=0.0      # Минимальное уменьшение нечистоты
)

# Обучение модели
clf.fit(X_train, y_train)

```

