Алгоритм случайного леса (Random Forest) является видом ансамблевого метода машинного обучения, который используется как для задач классификации, так и для регрессии. Основной принцип работы случайного леса заключается в комбинировании результатов нескольких деревьев решений для получения более точного и стабильного прогноза по сравнению с отдельными деревьями.

<h3>Основные шаги работы случайного леса :</h3>
<h6>Создание выборок :</h6>
Из обучающего набора данных случайным образом выбираются подмножества данных (с повторениями). Эти подмножества используются для построения каждого дерева решений в лесу. Обычно используется бутстрап 
<h6>Построение деревьев решений</h6>
На каждом подмножестве данных строится отдельное дерево решений. Каждое дерево строится независимо друг от друга.
<h6>Голосование или усреднение : </h6>Для задачи классификации результаты всех деревьев комбинируются путем голосования, а для задачи регрессии — путем усреднения предсказаний.

<h2>Регрессия</h2>
Итак, импортируем нужные библиотеки 

```python 
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, обработали категориальные данные [[Обработка категориальных данных]],  приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Как только наши данные подготовлены, можем приступить к обучению модели : 

```python 
RFR = RandomForestRegressor()
```

Далее, мы тренируем нашу модель : 

```python 
RFR.fit(X_train, y_train)
y_RFR_test_predict = RFR.predict(X_test)
```

<h3>Классификация</h6>

```python 
model = RandomForestClassifier()


model.fit(X_train, y_train) 
```

<h3>Гиперпараметры</h6>
<h6>Число деревьев (n_estimators) :</h6>
Количество деревьев в лесу. Большее число деревьев обычно улучшает точность модели, но увеличивает время обучения и предсказания.

<h6>Максимальная глубина деревьев (max_depth) : </h6>
Определяет максимальную глубину каждого дерева. Ограничение глубины помогает предотвратить переобучение и уменьшить сложность модели.

<h6>Минимальное количество образцов для разделения узла (min_samples_split) :</h6>
Минимальное количество образцов, необходимое для разбиения узла. Увеличение этого значения может помочь предотвратить переобучение.

<h6>Минимальное количество образцов в листьях (min_samples_leaf) : </h6> Минимальное количество образцов, которые должны находиться в каждом листе дерева. Это также помогает предотвратить переобучение, особенно в случае малых выборок.

<h6>Максимальное количество признаков для разбиения узла (max_features) :</h6>
Количество признаков, которые будут рассматриваться для разбиения узла. Можно использовать проценты от общего числа признаков или конкретное число признаков. Это помогает снизить корреляцию между деревьями и улучшить обобщающую способность модели.

<h6>Критерий разбиения (criterion) :</h6>
Функция, используемая для оценки качества разбиения узлов. Обычно используются "gini" (индекс Джини) или "entropy" (энтропия).

<h6>Bootstrap : </h6>
Логический параметр, который указывает, следует ли использовать выборку с возвратом для построения деревьев (если True) или без возврата (если False).

Например, у нас может получиться вот так : 

```python
param_grid = { 
			  'n_estimators': [10, 50, 100, 200], 
			  'max_depth': [None, 10, 20, 30], 
			  'min_samples_split': [2, 5, 10], 
			  'min_samples_leaf': [1, 2, 4], 
			  'max_features': ['auto', 'sqrt', 'log2'] 
			  }
```

Для нахождения оптимальны
