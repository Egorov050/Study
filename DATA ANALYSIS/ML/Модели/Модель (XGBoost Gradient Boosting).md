Для начала повторим принцип работы градиентного бустинга

Как работает градиентный бустинг?

Градиентный бустинг — это мощная техника машинного обучения, которая используется для построения предсказательных моделей. Принцип работы градиентного бустинга заключается в последовательном создании ансамбля слабых моделей (обычно деревьев решений) таким образом, чтобы каждая новая модель исправляла ошибки предыдущих. Вот как это работает шаг за шагом:
<h3>Принцип работы градиентного бустинга</h3>
<h6>Инициализация:</h6>
Начинаем с простой модели (обычно константы), которая предсказывает среднее значение целевой переменной для всех наблюдений в случае регрессии или распределение классов в случае классификации.

<h6>Вычисление ошибок:</h6>
На каждой итерации вычисляется ошибка (разница между реальными значениями и предсказанными значениями текущей модели). Эти ошибки называются остатками (residuals).

<h6>Обучение нового дерева:</h6>
Новая модель (обычно дерево решений) обучается на ошибках (остатках) предыдущей модели. Цель состоит в том, чтобы новая модель предсказывала эти ошибки как можно точнее.

<h6>Обновление предсказаний:</h6>
Обновляем предсказания, добавляя к текущим предсказаниям скорректированные значения, предсказанные новым деревом. Это делается с помощью взвешенного суммирования, где новый предсказатель умножается на некоторый коэффициент (скорость обучения, learning rate).

<h6>Повторение процесса:</h6>
Шаги 2-4 повторяются несколько раз (определённое количество итераций или до достижения определённого критерия остановки). На каждой итерации добавляется новое дерево, которое пытается скорректировать ошибки всех предыдущих деревьев.

Мы уже рассмотрели один из возможным методов градиентного бустинга [[Модель (lgb Gradient boosting )]] 

Теперь перейдем к еще одному способу. 
`XGBoost` - модель, которая используется для решения проблем классификации. 

`XGBoost` оптимизирован для скорости и производительности. Он включает различные методы регуляризации, чтобы избежать переобучения, эффективную обработку пропущенных данных и встроенную поддержку параллельной обработки.

https://www.youtube.com/watch?v=2Va5jMMBC0M

Импортируем `XGBoost` : 

```python 
import xgboost as xgb 
from xgboost import DMatrix
```

Что такое `DMatrix`?

`DMatrix` хранит данные в компактном формате, который оптимизирован для эффективного выполнения вычислений. Внутри XGBoost используется структура данных, похожая на разреженные матрицы (sparse matrices), что позволяет экономить память при работе с разреженными данными (например, при наличии большого количества нулевых значений).

В `DMatrix` от XGBoost входят как признаки (features), так и целевые переменные (target variables), но они передаются отдельно.

Мы можем получить эти матрицы следующим способом : 

```python
dtrain = DMatrix(X_train, label=y_train) 
dtest = DMatrix(X_test, label=y_test)
```

Параметры, которые мы задаем для `регрессии`: 

```python
params = { 
		  'objective': 'reg:squarederror', 
		  'max_depth': 6, 
		  'learning_rate': 0.1, 
		  'n_estimators': 100 }
		  
model = xgb.XGBRegressor(params)
```

Давайте разберём каждый из них:

`objective` : `'reg:squarederror'` :  Устанавливает целевую функцию, которую модель будет оптимизировать. В данном случае `reg:squarederror` означает, что задача — это регрессия с использованием квадратной ошибки (Mean Squared Error, MSE) в качестве метрики потерь. Применение : Подходит для задач регрессии, где требуется минимизировать среднюю квадратичную ошибку между предсказанными и фактическими значениями.

`max_depth`: `6`  : Устанавливает максимальную глубину каждого дерева решений. Глубина дерева определяет, сколько уровней будет иметь дерево от корня до самого удалённого узла (листа). Применение : Контролирует сложность модели. Более глубокие деревья могут захватывать более сложные зависимости, но также могут приводить к переобучению. Обычно используется для балансировки между сложностью модели и её способностью к обобщению.

`learning_rate` (или `eta`) : `0.1` :  Определяет темп обучения модели. Это значение указывает, насколько сильно каждое дерево будет корректировать ошибки предыдущих деревьев. Меньшие значения делают обучение более медленным, но более стабильным и точным. Применение : Задаёт скорость, с которой модель обновляет свои предсказания. Меньший `learning_rate` требует большего количества деревьев (`n_estimators`), чтобы достичь аналогичного уровня точности, но может привести к лучшим результатам, если правильно настроен.

`n_estimators` : `100` : Определяет количество деревьев, которые будут построены в процессе обучения модели. Это число указывает, сколько итераций бустинга будет выполнено. Применение : Определяет, сколько деревьев будет добавлено к ансамблю. Большее значение может улучшить точность модели, но при этом увеличивает время обучения и риск переобучения. 

Параметры, которые мы задаем для `классификации`: 

```python 
import xgboost as xgb

params = {
    'objective': 'multi:softmax',
    'num_class': 3,  # Количество классов
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100
}

model = xgb.XGBClassifier(params)

```

`objective='multi:softmax'` если нам нужно предсказать один из нескольких классов.

После того, как мы определили параметры, перейдем к тренировке : 

```python
model = xgb.train(params, dtrain, num_boost_round=100)
```

и к прогнозу : 

```python
y_pred = model.predict(dtest)
```

Появляется вопрос, `как выбрать лучшие параметры?` 

Один из возможных способов это использование `GridSearchCV`

`GridSearchCV` из библиотеки `scikit-learn` — это мощный инструмент для автоматического подбора гиперпараметров модели машинного обучения. Он позволяет систематически проверять набор параметров модели, чтобы найти наилучшие гиперпараметры, которые обеспечивают наилучшую производительность модели на основе кросс-валидации.

Для начала импортируем `GridSearchCV` : 

```python 
from sklearn.model_selection import GridSearchCV
```

Затем, зададим параметры , из которых нам нужно выбрать один сет гиперпараметров : 

```python
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

```

Далее, инициализируем модель и приступим к тренировке : 

```python 
# Инициализация модели 
model = xgb.XGBClassifier() 

# Настройка поиска по сетке с кросс-валидацией 
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy') grid_search.fit(X_train, y_train)
```

И далее выведем лучшие параметры : 

```python
print("Лучшие параметры:", grid_search.best_params_)
print("Лучший результат на обучающем наборе:", grid_search.best_score_)
```

И далее, уже данные параметры используем для нашей модели. 
