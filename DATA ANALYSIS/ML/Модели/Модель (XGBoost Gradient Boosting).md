Для начала повторим принцип работы градиентного бустинга

Как работает градиентный бустинг?

Градиентный бустинг — это мощная техника машинного обучения, которая используется для построения предсказательных моделей. Принцип работы градиентного бустинга заключается в последовательном создании ансамбля слабых моделей (обычно деревьев решений) таким образом, чтобы каждая новая модель исправляла ошибки предыдущих. Вот как это работает шаг за шагом:
<h3>Принцип работы градиентного бустинга</h3>
<h6>Инициализация:</h6>
Начинаем с простой модели (обычно константы), которая предсказывает среднее значение целевой переменной для всех наблюдений в случае регрессии или распределение классов в случае классификации.

<h6>Вычисление ошибок:</h6>
На каждой итерации вычисляется ошибка (разница между реальными значениями и предсказанными значениями текущей модели). Эти ошибки называются остатками (residuals).

<h6>Обучение нового дерева:</h6>
Новая модель (обычно дерево решений) обучается на ошибках (остатках) предыдущей модели. Цель состоит в том, чтобы новая модель предсказывала эти ошибки как можно точнее.

<h6>Обновление предсказаний:</h6>
Обновляем предсказания, добавляя к текущим предсказаниям скорректированные значения, предсказанные новым деревом. Это делается с помощью взвешенного суммирования, где новый предсказатель умножается на некоторый коэффициент (скорость обучения, learning rate).

<h6>Повторение процесса:</h6>
Шаги 2-4 повторяются несколько раз (определённое количество итераций или до достижения определённого критерия остановки). На каждой итерации добавляется новое дерево, которое пытается скорректировать ошибки всех предыдущих деревьев.

Мы уже рассмотрели один из возможным методов градиентного бустинга [[Модель (lgb Gradient boosting )]] 

Теперь перейдем к еще одному способу. 
`XGBoost` - модель, которая используется для решения проблем классификации. 

`XGBoost` оптимизирован для скорости и производительности. Он включает различные методы регуляризации, чтобы избежать переобучения, эффективную обработку пропущенных данных и встроенную поддержку параллельной обработки.

https://www.youtube.com/watch?v=2Va5jMMBC0M

Импортируем `XGBoost` : 

```python 
import xgboost as xgb 
from xgboost import DMatrix
```

Что такое `DMatrix`?

`DMatrix` хранит данные в компактном формате, который оптимизирован для эффективного выполнения вычислений. Внутри XGBoost используется структура данных, похожая на разреженные матрицы (sparse matrices), что позволяет экономить память при работе с разреженными данными (например, при наличии большого количества нулевых значений).

В `DMatrix` от XGBoost входят как признаки (features), так и целевые переменные (target variables), но они передаются отдельно.

Мы можем получить эти матрицы следующим способом : 

```python
dtrain = DMatrix(X_train, label=y_train) 
dtest = DMatrix(X_test, label=y_test)
```


