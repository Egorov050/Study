Для начала повторим принцип работы градиентного бустинга

Как работает градиентный бустинг?

Градиентный бустинг — это мощная техника машинного обучения, которая используется для построения предсказательных моделей. Принцип работы градиентного бустинга заключается в последовательном создании ансамбля слабых моделей (обычно деревьев решений) таким образом, чтобы каждая новая модель исправляла ошибки предыдущих. Вот как это работает шаг за шагом:
<h3>Принцип работы градиентного бустинга</h3>
<h6>Инициализация:</h6>
Начинаем с простой модели (обычно константы), которая предсказывает среднее значение целевой переменной для всех наблюдений в случае регрессии или распределение классов в случае классификации.

<h6>Вычисление ошибок:</h6>
На каждой итерации вычисляется ошибка (разница между реальными значениями и предсказанными значениями текущей модели). Эти ошибки называются остатками (residuals).

<h6>Обучение нового дерева:</h6>
Новая модель (обычно дерево решений) обучается на ошибках (остатках) предыдущей модели. Цель состоит в том, чтобы новая модель предсказывала эти ошибки как можно точнее.

<h6>Обновление предсказаний:</h6>
Обновляем предсказания, добавляя к текущим предсказаниям скорректированные значения, предсказанные новым деревом. Это делается с помощью взвешенного суммирования, где новый предсказатель умножается на некоторый коэффициент (скорость обучения, learning rate).

<h6>Повторение процесса:</h6>
Шаги 2-4 повторяются несколько раз (определённое количество итераций или до достижения определённого критерия остановки). На каждой итерации добавляется новое дерево, которое пытается скорректировать ошибки всех предыдущих деревьев.

Мы уже рассмотрели один из возможным методов градиентного бустинга [[Модель (lgb Gradient boosting )]] 

Теперь перейдем к еще одному способу. 
`XGBoost` - модель, которая используется для решения проблем классификации. 

`XGBoost` оптимизирован для скорости и производительности. Он включает различные методы регуляризации, чтобы избежать переобучения, эффективную обработку пропущенных данных и встроенную поддержку параллельной обработки.

https://www.youtube.com/watch?v=2Va5jMMBC0M

Импортируем `XGBoost` : 

```python 
import xgboost as xgb 
from xgboost import DMatrix
```

Что такое `DMatrix`?

`DMatrix` хранит данные в компактном формате, который оптимизирован для эффективного выполнения вычислений. Внутри XGBoost используется структура данных, похожая на разреженные матрицы (sparse matrices), что позволяет экономить память при работе с разреженными данными (например, при наличии большого количества нулевых значений).

В `DMatrix` от XGBoost входят как признаки (features), так и целевые переменные (target variables), но они передаются отдельно.

Мы можем получить эти матрицы следующим способом : 

```python
dtrain = DMatrix(X_train, label=y_train) 
dtest = DMatrix(X_test, label=y_test)
```

Параметры, которые мы задаем : 

В приведённом вами наборе параметров для XGBoost, каждый параметр отвечает за определённые аспекты конфигурации модели. Давайте разберём каждый из них:

`objective` : `'reg:squarederror'`
   - **Описание**: Устанавливает целевую функцию, которую модель будет оптимизировать. В данном случае `reg:squarederror` означает, что задача — это регрессия с использованием квадратной ошибки (Mean Squared Error, MSE) в качестве метрики потерь.
   - **Применение**: Подходит для задач регрессии, где требуется минимизировать среднюю квадратичную ошибку между предсказанными и фактическими значениями.

2. **`max_depth`**: `6`
   - **Описание**: Устанавливает максимальную глубину каждого дерева решений. Глубина дерева определяет, сколько уровней будет иметь дерево от корня до самого удалённого узла (листа).
   - **Применение**: Контролирует сложность модели. Более глубокие деревья могут захватывать более сложные зависимости, но также могут приводить к переобучению. Обычно используется для балансировки между сложностью модели и её способностью к обобщению.

3. **`learning_rate` (или `eta`)**: `0.1`
   - **Описание**: Определяет темп обучения модели. Это значение указывает, насколько сильно каждое дерево будет корректировать ошибки предыдущих деревьев. Меньшие значения делают обучение более медленным, но более стабильным и точным.
   - **Применение**: Задаёт скорость, с которой модель обновляет свои предсказания. Меньший `learning_rate` требует большего количества деревьев (`n_estimators`), чтобы достичь аналогичного уровня точности, но может привести к лучшим результатам, если правильно настроен.

4. **`n_estimators`**: `100`
   - **Описание**: Определяет количество деревьев, которые будут построены в процессе обучения модели. Это число указывает, сколько итераций бустинга будет выполнено.
   - **Применение**: Определяет, сколько деревьев будет добавлено к ансамблю. Большее значение может улучшить точность модели, но при этом увеличивает время обучения и риск переобучения. 

### Общее объяснение

В этом наборе параметров настроены ключевые аспекты модели XGBoost для задачи регрессии. Параметры определяют, как модель будет обучаться и какие метрики будут использоваться для оценки её производительности:

- **Целевая функция** (`objective`) задаёт, как будет измеряться ошибка модели.
- **Глубина дерева** (`max_depth`) влияет на сложность модели.
- **Темп обучения** (`learning_rate`) управляет тем, как быстро модель обучается.
- **Количество деревьев** (`n_estimators`) задаёт количество итераций или этапов обучения.

Эти параметры следует подбирать в зависимости от конкретной задачи и данных. Настройка гиперпараметров может требовать экспериментов и валидации для достижения наилучших результатов.
