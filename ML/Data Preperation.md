Перед тем, как применять различные ml подели да и вообще работать с данными, нам нужно привести их в нормальное состояние. Рассмотрим несколько шагов.

<h2>Очистка данных</h2>
Удаление пропущенных значений:
-  *Удаление строк или столбцов* с пропущенными значениями.
- *Замена пропущенных значений* на средние, медианные или наиболее часто встречающиеся значения, или использование более сложных методов, таких как иммпутация. 
- *удаление дубликатов*

Делаем при помощи :
[]


<h2>Обработка категориальных данных (One hot encoding)</h2>
Для этого мы используем one hot encoding. Делается это вот таким способом : 

```python 
import pandas as pd

data = {
    'Марка машины': ['Toyota', 'Honda', 'BMW', 'Toyota', 'BMW', 'Honda'],
    'Год производства': [2010, 2012, 2010, 2018, 2012, 2018]
}
df = pd.DataFrame(data)


one_hot_encoded = pd.get_dummies(df, columns=['Марка машины', 'Год производства'])

one_hot_encoded = one_hot_encoded.astype(int)

print(one_hot_encoded)
```

Разберем этот код : 
Понятно, что мы создаем датафрейм и тд. 
По факту, мы используем функцию `pd.get_dummies` где уточняем наш датасет и колонки, которые нужно заинкодить:

```python
one_hot_encoded = pd.get_dummies(df, columns=['Марка машины', 'Год производства'])
```

Однако, после такого у нас будет вот такая таблица : 

```python 
Марка машины_BMW  Марка машины_Honda  Марка машины_Toyota  \
0             False               False                 True   
1             False                True                False   
2              True               False                False   
3             False               False                 True   
4              True               False                False   
5             False                True                False   

   Год производства_2010  Год производства_2012  Год производства_2018  
0                   True                  False                  False  
1                  False                   True                  False  
2                   True                  False                  False  
3                  False                  False                   True  
4                  False                   True                  False  
5                  False                  False                   True  
```

Так как нам нужны 0 и 1 , то мы прописываем обычную функцию, которая преобразует данные в цифры : 

```python 
one_hot_encoded = one_hot_encoded.astype(int)
```

Тогда мы получаем : 

```python
Марка машины_BMW  Марка машины_Honda  Марка машины_Toyota  \
0                 0                   0                    1   
1                 0                   1                    0   
2                 1                   0                    0   
3                 0                   0                    1   
4                 1                   0                    0   
5                 0                   1                    0   

   Год производства_2010  Год производства_2012  Год производства_2018  
0                      1                      0                      0  
1                      0                      1                      0  
2                      1                      0                      0  
3                      0                      0                      1  
4                      0                      1                      0  
5                      0                      0                      1
```

<h2>Масштабирование признаков</h2> 
Масштабирование признаков заключается в преобразовании данных таким образом, чтобы они имели схожие масштабы.

Масштабирование признаков необходимо для некоторых моделей машинного обучения, чтобы улучшить их производительность, численную стабильность и скорость сходимости. Ниже приведены категории моделей, которые требуют или не требуют масштабирования признаков.

<h3>Модели, которые нужно масштабировать признаки : </h3>
<h4>Методы на основе расстояния :</h4>
1) K-Nearest Neighbors (KNN) []: Использует евклидово расстояние для определения ближайших соседей. Несмасштабированные признаки могут значительно исказить результаты.
2) Кластеризация (например, K-Means): Расстояния между центроидами и точками данных зависят от масштаба признаков.

<h4>Линейные модели :</h4>
1) Логистическая регрессия и линейная регрессия: Масштабирование улучшает численную стабильность и скорость сходимости оптимизационных алгоритмов.


<h3>Модели, не требующие масштабирования</h3>
<h4>Деревья решений : </h4>
1) Decision Trees: Делают разбиения на основе порогов, поэтому масштаб данных не влияет на процесс разбиения.
2) Random Forest: Множество деревьев решений, не чувствительных к масштабу признаков.
3) Gradient Boosting: Так же как и деревья решений, не чувствителен к масштабу признаков.

<h2>Как мы это делаем?</h2>
```python 
# Инициализация 
StandardScaler scaler = StandardScaler() 
# Обучение scaler на тренировочных данных 
scaler.fit(X_train) 
# Преобразование тренировочных данных 
X_train_normed = scaler.transform(X_train) 
# Преобразование тестовых данных 
X_test_normed = scaler.transform(X_test)
```


*Примечание : Масштабирование числовых признаков после One-Hot Encoding не влияет на закодированные категориальные признаки. При масштабировании данные, полученные после one-hot encoding (столбцы с 0 и 1), останутся в неизменном виде, поскольку масштабирование не изменяет значения 0 и 1. Оно лишь приводит числовые признаки к заданному диапазону.*


