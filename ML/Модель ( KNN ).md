K-Nearest Neighbors (KNN) — это один из простых и понятных алгоритмов машинного обучения, используемый как для задач классификации, так и для задач регрессии. Основная идея KNN заключается в том, что объект классифицируется на основе большинства его k ближайших соседей в пространстве признаков. Вот основные аспекты KNN:
<h6>Принцип работы :</h6>
При классификации новый объект относится к тому классу, который является наиболее частым среди k ближайших соседей.

При регрессии значение для нового объекта вычисляется как среднее значение его k ближайших соседей.

![[Pasted image 20240615131901.png]]


<h2>Регрессия</h2>
Итак, импортируем нужные библиотеки 

```python 
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, обработали категориальные данные [[Обработка категориальных данных]],  приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Как только наши данные подготовлены, можем приступить к обучению модели : 

```python 
kn = KNeighborsRegressor()
kn.fit(X_train, y_train)
```

Модель наша обучилась, далее мы засовываем `X_test` в нашу модель , мы получим `y_predict` и затем сравниваем `y_predict` с `y_test` : 

```python 
y_pred = kn.predict(X_test)
```

Затем, мы можем смотреть результат используя метрики [[Метрики]]


<h2>Классификация</h2>
```python
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NeighborsClassifier
```

Далее мы подготавливаем наши данные см ( [[Подготовка данных]] )
Как только мы почистили , привели данные в порядок, обработали категориальные данные [[Обработка категориальных данных]],  приступим к сплитованию :
см ( [[Сплит данных перед моделью]] )

Как только наши данные подготовлены, можем приступить к обучению модели : 

```python 
kn = NeighborsClassifier()
kn.fit(X_train, y_train)
```

Модель наша обучилась, далее мы засовываем `X_test` в нашу модель , мы получим `y_predict` и затем сравниваем `y_predict` с `y_test` : 

```python 
y_pred = kn.predict(X_test)
```

Затем, мы можем смотреть результат используя метрики [[Метрики]]

В методе k-ближайших соседей (kNN, k-nearest neighbors), решение для нового объекта определяется на основе ближайших к нему объектов из обучающего набора данных. Вот как это происходит в зависимости от типа задачи:

### В случае регрессии:

1. **Нахождение k ближайших соседей:**
   - Для нового объекта сначала находятся k ближайших соседей из обучающего набора. Расстояние до соседей обычно вычисляется с использованием евклидова расстояния или других метрик расстояния, определенных пользователем.

2. **Выбор ответа:**
   - В случае регрессии ответ для нового объекта определяется путем усреднения ответов (целевых переменных) всех k ближайших соседей.
   - Формально это выглядит так:
     \[ \hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i \]
     где \( \hat{y} \) — предсказанный ответ для нового объекта,
     \( y_i \) — ответ (целевая переменная) для i-го ближайшего соседа.

### В случае классификации:

1. **Нахождение k ближайших соседей:**
   - Также находятся k ближайших соседей для нового объекта.

2. **Выбор ответа:**
   - В случае классификации ответ для нового объекта выбирается путем голосования среди классов k ближайших соседей.
   - Это означает, что каждый сосед "голосует" за свой класс, и класс с наибольшим количеством голосов становится предсказанным классом для нового объекта.

### Пример:

Предположим, что у нас есть обучающий набор данных для задачи регрессии или классификации. После того, как мы нашли k ближайших соседей для нового объекта, мы используем описанные выше методы для определения ответа.

Например, в задаче регрессии, если у нас есть k=3 ближайших соседа с ответами 5, 7, 4, то предсказанный ответ для нового объекта будет:

\[ \hat{y} = \frac{1}{3} \cdot (5 + 7 + 4) = 5.33 \]

В задаче классификации, если у нас есть k=5 ближайших соседей с классами 'кот', 'собака', 'кот', 'собака', 'кот', то новый объект будет отнесен к классу 'кот', так как 'кот' получил больше голосов.

Таким образом, kNN использует усреднение для регрессии и голосование для классификации, чтобы предсказать ответы для новых объектов на основе ближайших к ним объектов из обучающего набора данных.