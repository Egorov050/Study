**Evaluation metrics** are used to measure the quality of the model. One of the most important topics in machine learning is how to evaluate your model. When you build your model, it is very crucial to measure how accurately it predicts your expected outcome.

<h2>Confusion Matrix</h2>
A confusion matrix is a table that summarizes the performance of a classifier by showing the number of true positive, true negative, false positive, and false negative predictions. It is useful to understand the strengths and weaknesses of a classifier, and also provides information about the distribution of errors made by the classifier:

| True Label \ Predicted Label | Positive | Negative |
| ---------------------------- | -------- | -------- |
| Positive                     | TP       | FN       |
| Negative                     | FP       | TN       |

Where:
`TP (True Positive)` - The model correctly predicted the positive class.
`TN (True Negative)` - The model correctly predicted the negative class.
`FP (False Positive)` - The model predicted the positive class, but it's actually negative.
`FN (False Negative)` - The model predicted the negative class, but it's actually positive.

<h6>Как мы можем реализовать ее на питоне?</h6>

```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(LGB_model.predict(X_test), y_test)
disp = plot_confusion_matrix(cm)
disp.plot()
plt.show()
```


<h2>Accuracy</h2>
This metric is defined as the ratio of correctly predicted instances to the total number of instances in the dataset. It measures how often the classifier is correct. However, it can be misleading in imbalanced datasets, as it may give high accuracy scores even if the classifier is not able to accurately predict the minority class. Based on the confusion matrix shows, the accuracy is computed as:

$$
Accuracy = \frac{TP + FN}{TP + FN + TN + FP}
$$

Другими словами : Accuracy (точность) в контексте машинного обучения представляет собой метрику, которая измеряет долю правильных предсказаний модели среди всех предсказаний, сделанных моделью.

Чем выше значение accuracy, тем лучше модель выполняет предсказания. Однако, следует учитывать следующие моменты:

1. **Чувствительность к классам**: Если классы в данных несбалансированы (то есть один класс преобладает по количеству наблюдений), высокая accuracy может быть обусловлена простым предсказанием самого распространенного класса.

2. **Недостатки**: Accuracy не учитывает тип ошибок (ложноположительные и ложноотрицательные результаты) и не информирует о качестве предсказаний внутри каждого класса.

3. **Контекст использования**: В некоторых задачах (например, детекция мошенничества или редких событий) accuracy может быть менее информативной метрикой, чем другие метрики, такие как precision, recall или F1-score.


<h2>Precision</h2>
Precision is the ratio of correctly predicted positive instances to the total number of instances predicted as positive. It is a measure of the classifier's ability to correctly identify positive instances and avoid false positives.

$$
Precision = \frac{TP}{TP + FP}
$$

Другими словами, Precision (точность) — это метрика, которая измеряет, насколько точно модель предсказывает положительный класс. Она показывает долю истинных положительных предсказаний среди всех предсказаний положительного класса : 

```python 
from sklearn.metrics import precision_score

# Пример истинных значений и предсказаний
y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]

# Расчет precision
precision = precision_score(y_true, y_pred)

print(f'Precision: {precision}')

Precision: 0.8
```

Если точность равна 65%, это означает, что из всех случаев, которые модель предсказала как положительные (то есть предсказала, что человек болеет COVID-19), 65% действительно болеют COVID-19.

<h2>Recall</h2>
Recall is the ratio of correctly predicted positive instances to the total number of actual positive instances. It is a measure of the classifier's ability to detect all positive instances.

$$
Recall = \frac{TP}{TP + FN}
$$

Recall показывает долю правильных положительных предсказаний среди всех объектов, которые действительно являются положительными. Это метрика, которая показывает, как много из предсказанных моделью положительных примеров действительно являются положительными.

```python 
from sklearn.metrics import recall_score

# Пример истинных значений и предсказаний
y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]

# Расчет recall
recall = recall_score(y_true, y_pred)

print(f'Recall: {recall}')
```

Если полнота (recall или sensitivity) равна 65%, это значит, что из всех реальных случаев заболевания COVID-19, модель правильно идентифицирует 65% этих случаев.

<h2>F1 - score TUTUTUTU MAX VERSTAPPEN</h2>
F1-score является метрикой, которая объединяет точность (precision) и полноту (recall) в одно значение. Это гармоническое среднее точности и полноты, и оно особенно полезно, когда важен баланс между ними, например, когда классы несбалансированы.

$$
F1 score = 2*\frac{Precision*Recall }{Precision + Recall}
$$

F1-score полезен, когда нужно учитывать оба аспекта: и способность модели находить все положительные случаи (полнота), и точность предсказаний (точность). Например, в медицинских тестах важно как выявлять все случаи болезни, так и минимизировать ложные срабатывания.

```python
from sklearn.metrics import f1_score

y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]

f1_score = f1_score(y_true, y_pred)
```

<h2>MSE</h2>
MSE (Mean Squared Error) или среднеквадратичная ошибка — это одна из наиболее распространенных метрик для оценки качества модели в задачах регрессии в машинном обучении. Она показывает среднее значение квадратов отклонений предсказанных значений модели от фактических значений в тестовом наборе данных.
<h3>Формула MSE</h3>
Для набора данных с n примерами, где yi​ — фактическое значение, y^i(hat)— предсказанное значение моделью, MSE вычисляется следующим образом:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

<h6>Интерпретация</h6>
- **Чем меньше MSE, тем лучше модель.** Меньшее значение MSE указывает на то, что модель лучше справляется с предсказанием значений.
    
- **Численное значение MSE:** МSE выражается в терминах квадратных единиц измерения целевой переменной. Например, если мы предсказываем цены на недвижимость в долларах, то MSE будет измеряться в квадратах долларов.

На питоне, мы можем найти следующим образом : 

```python 
from sklearn.metrics import mean_squared_error, mean_absolute_error, max_error
print("MSE: ", mean_squared_error(y_test, y_pred))
```


<h2>ROC-AUC: Подробное объяснение</h2>
**ROC-AUC** — это метрика, используемая для оценки качества бинарных классификаторов. ROC-кривая (Receiver Operating Characteristic curve) и AUC (Area Under the Curve) часто используются вместе для измерения способности модели различать между двумя классами.

**AUC (Area Under the Curve)** — это площадь под ROC-кривой. Она варьируется от 0 до 1:

- AUC = 1.0 указывает на идеальную модель.
- AUC = 0.5 указывает на случайную модель.
- Чем выше AUC, тем лучше модель.

Хорошо, давайте рассмотрим пример с данными, где потребуется группировка элементов с одинаковыми вероятностями.

### Пример данных с одинаковыми вероятностями

| Пример | Вероятность | Фактическая метка |
|--------|-------------|-------------------|
| 1      | 0.9         | 1                 |
| 2      | 0.7         | 1                 |
| 3      | 0.6         | 0                 |
| 4      | 0.6         | 1                 |
| 5      | 0.4         | 0                 |
| 6      | 0.2         | 0                 |
| 7      | 0.1         | 0                 |

### Шаги по алгоритму

#### 1. Получение вероятностей
Используем предсказанные вероятности из примера.

#### 2. Деление отрезка [0,1]
Количество частей на оси x равно количеству элементов отрицательного класса (4).
Количество частей на оси y равно количеству элементов положительного класса (3).

#### 3. Сортировка объектов по вероятностям
Сортируем объекты по вероятностям в порядке убывания:

| Пример | Вероятность | Фактическая метка |
|--------|-------------|-------------------|
| 1      | 0.9         | 1                 |
| 2      | 0.7         | 1                 |
| 3      | 0.6         | 0                 |
| 4      | 0.6         | 1                 |
| 5      | 0.4         | 0                 |
| 6      | 0.2         | 0                 |
| 7      | 0.1         | 0                 |

#### 4. Группировка элементов с одинаковыми вероятностями
Группируем элементы с одинаковыми вероятностями:

| Вероятность | Фактическая метка |
|-------------|--------------------|
| 0.9         | 1                 |
| 0.7         | 1                 |
| 0.6         | 0, 1              |
| 0.4         | 0                 |
| 0.2         | 0                 |
| 0.1         | 0                 |

#### 5. Проход по таблице с вероятностями

Мы будем двигаться по таблице и строить ROC-кривую. Начальная точка \([0,0]\).

| Вероятность | Фактическая метка | Действие      | Новая координата |
|-------------|--------------------|---------------|------------------|
| 0.9         | 1                  | Вверх         | (0, 1/3)         |
| 0.7         | 1                  | Вверх         | (0, 2/3)         |
| 0.6         | 0, 1               | Диагональ     | (1/4, 1)         |
| 0.4         | 0                  | Вправо        | (2/4, 1)         |
| 0.2         | 0                  | Вправо        | (3/4, 1)         |
| 0.1         | 0                  | Вправо        | (1, 1)           |

#### Итоговая ROC-кривая

Начальная точка — \([0, 0]\), конечная точка — \([1, 1]\).

### Визуализация шагов

1. Начинаем с \([0, 0]\).
2. Вероятность 0.9, метка 1: шаг вверх к \([0, 1/3]\).
3. Вероятность 0.7, метка 1: шаг вверх к \([0, 2/3]\).
4. Вероятность 0.6, метка 0 и 1: диагональный шаг к \([1/4, 1]\).
5. Вероятность 0.4, метка 0: шаг вправо к \([2/4, 1]\).
6. Вероятность 0.2, метка 0: шаг вправо к \([3/4, 1]\).
7. Вероятность 0.1, метка 0: шаг вправо к \([1, 1]\).

### Расчет площади под кривой (AUC)

AUC можно рассчитать как сумму площадей трапеций под ROC-кривой.

#### Площадь трапеции

Площадь каждой трапеции можно вычислить как:

$$
\text{Площадь} = \frac{(b_1 + b_2) \cdot h}{2}
$$

Где \( b_1 \) и \( b_2 \) — длины параллельных сторон трапеции, а \( h \) — высота трапеции.

В нашем примере:

1. Трапеция от \([0, 0]\) до \([0, 1/3]\): площадь = 0 (нет ширины).
2. Трапеция от \([0, 1/3]\) до \([0, 2/3]\): площадь = 0 (нет ширины).
3. Трапеция от \([0, 2/3]\) до \([1/4, 1]\): площадь = \((1/4) \cdot (1 - 2/3) / 2 = 1/24\).
4. Трапеция от \([1/4, 1]\) до \([2/4, 1]\): площадь = \((1/4) \cdot (1 - 1) / 2 = 0\).
5. Трапеция от \([2/4, 1]\) до \([3/4, 1]\): площадь = \((1/4) \cdot (1 - 1) / 2 = 0\).
6. Трапеция от \([3/4, 1]\) до \([1, 1]\): площадь = \((1/4) \cdot (1 - 1) / 2 = 0\).

Суммируем все площади трапеций:

$$
AUC = 0 + 0 + 1/24 + 0 + 0 + 0 = 1/24 \approx 0.0417
$$

Итак, мы вручную прошли через процесс построения ROC-кривой и вычисления AUC для заданного примера.

ROC-AUC позволяет понять, насколько хорошо классификатор способен различать два класса.

<h2>MAE</h2>
Mean Absolute Error (MAE), или средняя абсолютная ошибка, является ещё одной распространённой метрикой в задачах регрессии. Она измеряет среднее абсолютное отклонение предсказанных значений модели от фактических значений в тестовом наборе данных.

Формула выглядит следующим образом : 

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

MAE применяется в тех же случаях, что и MSE, однако она более устойчива к выбросам из-за своей линейной природы.


<h2>Classification Report</h2>
Classification Report (отчет о классификации) — это статистический отчет, который используется для оценки качества работы алгоритма классификации на тестовом наборе данных. Он содержит информацию о различных метриках оценки, таких как точность (precision), полнота (recall), F1-мера (F1-score) и поддержка (support), а также может включать информацию о других метриках, таких как матрица ошибок (confusion matrix)
\
Как мы можем это сделать? 

```python 
from sklearn.metrics import classification_report
```

Когда мы уже натренировали модель и уже готовы смотреть на метрики,  мы пишем : 

```python 
print(classification_report(y_test, LGB_model.predict(X_test)))
```

<h2>Почему важно сравнивать метрики  train и test семпла?</h2>
Сравнение метрик на тестовом и тренировочном наборах данных после обучения модели даёт важную информацию о её качестве и способности к обобщению:

<h6>Оценка переобучения</h6>
Сравнение метрик на тренировочных и тестовых данных помогает оценить, произошло ли переобучение модели. Переобучение возникает, когда модель слишком хорошо "запоминает" тренировочные данные и не способна хорошо обобщать на новых данных. Если метрики на тренировочном наборе данных значительно лучше, чем на тестовом, это может свидетельствовать о переобучении.

### 1. K-Nearest Neighbors (KNN)

**Признаки переобучения:**

- **Высокая точность на тренировочном наборе данных**: KNN может легко "запоминать" обучающие примеры, особенно при малом значении параметра `k`, что приводит к высокой точности на тренировочном наборе.
    
- **Низкая точность на тестовом наборе данных**: Если модель показывает существенно более низкую точность на тестовом наборе данных по сравнению с тренировочным, это может указывать на переобучение.
    
- **Вариация в метриках при изменении `k`**: При увеличении значения `k` может произойти улучшение обобщающей способности модели и снижение риска переобучения.
    

### 2. Logistic Regression

**Признаки переобучения:**

- **Высокая точность на тренировочном наборе данных**: Logistic Regression также может показать высокую точность на тренировочных данных, особенно если данные линейно разделимы.
    
- **Различия в точности между тренировочным и тестовым наборами**: Значительные различия в метриках (например, точность, F1-мера) между тренировочным и тестовым наборами данных могут указывать на переобучение.
    
- **Разреженность коэффициентов**: В случае логистической регрессии переобучение может проявляться в том, что некоторые коэффициенты модели становятся слишком большими или слишком маленькими, что может быть признаком переобучения.
    

### 3. Decision Tree

**Признаки переобучения:**

- **Глубокое дерево**: Если дерево имеет слишком большую глубину, оно может идеально подстроиться под обучающие данные, что приведет к высокой точности на тренировочном наборе данных, но низкой на тестовом.
    
- **Многочисленные листья и ветви**: Слишком сложное дерево с множеством листьев и ветвей может указывать на переобучение.
    
- **Очень высокая точность на тренировочном наборе данных**: При глубоком дереве модель может показывать почти идеальные результаты на тренировочных данных.
    

### 4. Gradient Boosting

**Признаки переобучения:**

- **Слишком большое количество деревьев**: Если использовать слишком много базовых моделей (деревьев) в градиентном бустинге, модель может переобучиться на тренировочных данных и показать плохую обобщающую способность.
    
- **Медленное снижение ошибки на тестовом наборе**: При переобучении градиентный бустинг может показать медленное снижение ошибки на тестовом наборе данных или даже её увеличение после определенного числа итераций.
    
- **Метрики на тренировочных данных лучше, чем на тестовых**: Существенные различия в метриках на тренировочном и тестовом наборах данных могут свидетельствовать о переобучении.
    

### 5. Random Forest

**Признаки переобучения:**

- **Большое количество деревьев**: Если увеличить число деревьев в случайном лесе, модель может начать переобучаться, "запоминая" тренировочные данные.
    
- **Метрики на тренировочном наборе лучше, чем на тестовом**: Как и в случае с градиентным бустингом, различия в метриках между тренировочным и тестовым наборами данных могут свидетельствовать о переобучении.
    
- **Глубокие деревья**: Если деревья в случайном лесе имеют слишком большую глубину или слишком много признаков для разделения, это также может привести к переобучению.

<h6>Оценка качества модели </h6>
Сравнение метрик (например, точности, полноты, F1-меры и т.д.) на тренировочных и тестовых данных помогает оценить общее качество модели. Если метрики на тестовом наборе данных высоки и близки к метрикам на тренировочном наборе, это может указывать на то, что модель хорошо обучена и способна обобщать данные.

<h6>Идентификация проблем</h6>
Разница в метриках на тренировочном и тестовом наборах данных может также указывать на проблемы в данных или модели. Например, низкие метрики на тестовом наборе могут говорить о несбалансированности данных, переобучении или недообучении модели.