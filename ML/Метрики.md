**Evaluation metrics** are used to measure the quality of the model. One of the most important topics in machine learning is how to evaluate your model. When you build your model, it is very crucial to measure how accurately it predicts your expected outcome.

<h2>Confusion Matrix</h2>
A confusion matrix is a table that summarizes the performance of a classifier by showing the number of true positive, true negative, false positive, and false negative predictions. It is useful to understand the strengths and weaknesses of a classifier, and also provides information about the distribution of errors made by the classifier:

| True Label \ Predicted Label | Positive | Negative |
| ---------------------------- | -------- | -------- |
| Positive                     | TP       | FN       |
| Negative                     | FP       | TN       |

Where:
`TP (True Positive)` - The model correctly predicted the positive class.
`TN (True Negative)` - The model correctly predicted the negative class.
`FP (False Positive)` - The model predicted the positive class, but it's actually negative.
`FN (False Negative)` - The model predicted the negative class, but it's actually positive.


<h2>Accuracy</h2>
This metric is defined as the ratio of correctly predicted instances to the total number of instances in the dataset. It measures how often the classifier is correct. However, it can be misleading in imbalanced datasets, as it may give high accuracy scores even if the classifier is not able to accurately predict the minority class. Based on the confusion matrix shows, the accuracy is computed as:

$$
Recall = \frac{TP + FN}{TP + FN + TN + FP}
$$

Другими словами : Accuracy (точность) в контексте машинного обучения представляет собой метрику, которая измеряет долю правильных предсказаний модели среди всех предсказаний, сделанных моделью.

Чем выше значение accuracy, тем лучше модель выполняет предсказания. Однако, следует учитывать следующие моменты:

1. **Чувствительность к классам**: Если классы в данных несбалансированы (то есть один класс преобладает по количеству наблюдений), высокая accuracy может быть обусловлена простым предсказанием самого распространенного класса.

2. **Недостатки**: Accuracy не учитывает тип ошибок (ложноположительные и ложноотрицательные результаты) и не информирует о качестве предсказаний внутри каждого класса.

3. **Контекст использования**: В некоторых задачах (например, детекция мошенничества или редких событий) accuracy может быть менее информативной метрикой, чем другие метрики, такие как precision, recall или F1-score.


<h2>Precision</h2>
Precision is the ratio of correctly predicted positive instances to the total number of instances predicted as positive. It is a measure of the classifier's ability to correctly identify positive instances and avoid false positives.

$$
Precision = \frac{TP}{TP + FP}
$$

Другими словами, Precision (точность) — это метрика, которая измеряет, насколько точно модель предсказывает положительный класс. Она показывает долю истинных положительных предсказаний среди всех предсказаний положительного класса : 

```python 
from sklearn.metrics import precision_score

# Пример истинных значений и предсказаний
y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]

# Расчет precision
precision = precision_score(y_true, y_pred)

print(f'Precision: {precision}')

Precision: 0.8
```

Если точность равна 65%, это означает, что из всех случаев, которые модель предсказала как положительные (то есть предсказала, что человек болеет COVID-19), 65% действительно болеют COVID-19.

<h2>Recall</h2>
Recall is the ratio of correctly predicted positive instances to the total number of actual positive instances. It is a measure of the classifier's ability to detect all positive instances.

$$
Recall = \frac{TP}{TP + FN}
$$

Recall показывает долю правильных положительных предсказаний среди всех объектов, которые действительно являются положительными. Это метрика, которая показывает, как много из предсказанных моделью положительных примеров действительно являются положительными.

```python 
from sklearn.metrics import recall_score

# Пример истинных значений и предсказаний
y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]

# Расчет recall
recall = recall_score(y_true, y_pred)

print(f'Recall: {recall}')
```

Если полнота (recall или sensitivity) равна 65%, это значит, что из всех реальных случаев заболевания COVID-19, модель правильно идентифицирует 65% этих случаев.

<h2>F1 - score</h2>
$$
F1 score = 2*\frac{Precision*Recall }{Precision + Recall}
$$