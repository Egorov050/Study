Linear space - A mathematical structure is a set of elements called vectors, for which the operations of addition with each other and multiplication by a number are defined - a scalar

In every linear space, there are several principal vectors called the basis. All other vectors can be expressed through these basis vectors. A basis is a minimal set of vectors such that:
1) None of them can be expressed as a linear combination of the others;
2) Every element of the space can be uniquely represented as a linear combination of a finite set of vectors from this set.

Определитель — числовая характеристика матрицы, которая в определенном смысле описывает сжатие или расширение пространства при её преобразовании.
Если мы связываем полученное значение с его геометрическим значением, учитывая, что мы преобразуем единичный квадрат с помощью матрицы:

- Отрицательный знак указывает на то, что наш квадрат будет иметь противоположную ориентацию после преобразования. Это означает, что оси координат в квадрате будут развернуты.

- Значение 2 указывает на то, что после преобразования площадь нашего квадрата будет в два раза больше, чем у исходного квадрата. Это говорит о том, что преобразование расширяет пространство в два раза по площади.

Feature matrix (матрица признаков) — это основная структура данных в задачах машинного обучения и анализа данных. Она представляет собой двумерную матрицу, где каждая строка соответствует отдельному наблюдению (или объекту), а каждый столбец представляет собой признак (feature), который описывает данный объект.

Айгенвектор — это вектор, который при преобразовании матрицей AAA сохраняет своё направление (или противоположное направление), но может изменяться его масштаб (длина).

### Шаги алгоритма PCA:

1. **Нахождение направления максимальной дисперсии:**
    
    Представьте, что у вас есть набор данных, который представляет собой облако точек в двумерном пространстве (например, набор из двух признаков). Цель PCA — найти направление (главную компоненту), вдоль которого данные имеют наибольшую дисперсию.
    
    На этом шаге PCA находит такое направление (главную компоненту), вдоль которого разброс точек (дисперсия) максимален. Это направление описывает основное направление изменчивости данных.
    
2. **Нахождение следующих главных компонент:**
    
    После того как найдена первая главная компонента, следующие компоненты ищутся среди оставшихся направлений, ортогональных (перпендикулярных) уже найденным компонентам. Каждая последующая компонента находится так, чтобы максимизировать оставшуюся дисперсию данных.
    
    На рисунке выше показаны две главные компоненты: первая (прямая линия) и вторая (перпендикулярная к первой). Вторая компонента ортогональна первой и описывает второе по величине направление изменчивости данных.
    
3. **Повторение для всех компонент:**
    
    Процесс повторяется до тех пор, пока не будет найдено определенное число главных компонент или пока не будет достигнута определенная часть дисперсии, которую мы хотим сохранить.

An orthogonal matrix is a matrix for which the inverse matrix is equal to its transpose.

Error function (функция ошибки) обычно относится к математической функции, которая используется для измерения ошибки или расхождения между фактическими и предсказанными значениями в задачах моделирования, статистики или машинного обучения.

1. **В контексте статистики и машинного обучения:** Error function часто представляет собой функцию, которая выражает расхождение между прогнозами модели и наблюдаемыми данными. Например, в задачах регрессии часто используется среднеквадратичная ошибка (Mean Squared Error, MSE) или средняя абсолютная ошибка (Mean Absolute Error, MAE) в качестве error function.
Основная цель - минимизация error function!

Предел функции означает, что значение функции f(x)приближается к числу L столь близко, насколько это возможно, когда x находится достаточно близко к a, но x не равно a.

Функция называется непрерывной (continuous), если она сохраняет свои значения без разрывов и прерываний. 
A function is said to be continuous on a set if it is continuous at every point of that set (the set can be a simple segment on the axis, for example).

Если функция изменяется почти так, как линейная функция вблизи определённой точки, то говорят, что она дифференцируема в этой точке.

Градиентный вектор — это вектор, который состоит из частных производных функции по каждой из её переменных. Он показывает направление наибольшего возрастания функции в данной точке.

$$
Функция потерь \ J(\theta):
$$

$$
J(\theta) = (\theta - 5)^2
$$

$$
Градиент функции потерь \ \nabla J(\theta):
$$

$$
\nabla J(\theta) = \frac{\partial J(\theta)}{\partial \theta} = 2(\theta - 5)
$$

$$
Итерационное обновление параметра \ \theta \ с использованием градиентного спуска:
$$

$$
\theta := \theta - \alpha \nabla J(\theta)
$$

Здесь \( \alpha \) — скорость обучения.

Итерации градиентного спуска:

1. **Итерация 1:**

$$
\theta := 0 - 0.2 \cdot (0 - 5) = 0 + 1 = 1
$$

2. **Итерация 2:**

$$
\theta := 1 - 0.2 \cdot (1 - 5) = 1 + 0.8 = 1.8
$$

3. **Итерация 3:**

$$
\theta := 1.8 - 0.2 \cdot (1.8 - 5) = 1.8 + 0.84 = 2.64
$$

4. **Итерация 4:**

$$
\theta := 2.64 - 0.2 \cdot (2.64 - 5) = 2.64 + 0.672 = 3.312
$$


И так далее. Каждая итерация приближает \( \theta \) к значению \( 5 \), минимизируя функцию потерь \( J(\theta) \).
 be calculated through the dot product of the gradient with the directional vector of that direction.

Градиентный спуск : 

![[Screenshot 2024-06-23 at 19.33.50.png]]

![[Screenshot 2024-06-23 at 19.33.59.png]]

Метрическое пространство (Metric space) в математике — это абстрактное математическое понятие, которое представляет собой пару из множества объектов и функции, называемой метрикой, которая определяет расстояние между этими объектами.

Гиперплоскость (hyperplane) — это обобщение понятия плоскости на пространства любой размерности. В математике и особенно в линейной алгебре гиперплоскость представляет собой подпространство, размерность которого на одну меньше размерности самого пространства.

Линейная зависимость между двумя переменными означает, что одна переменная изменяется пропорционально другой с постоянной скоростью.

Нелинейная зависимость между переменными означает, что связь между ними не может быть выражена прямой линией.

Для регрессионных задач понятие неоднородности (heterogeneity) оценивается как дисперсия целевой переменной в выборке. Дисперсия представляет собой меру разброса или сходства данных в выборке.

Для задач классификации понятие неоднородности выборки связано с пропорциями классов в ней: если какой-то класс доминирует, то неоднородность будет низкой, а если распределение классов примерно равномерно, то неоднородность будет высокой.

Индекс Джини (Gini Index) и энтропия (Entropy) — это метрики, используемые в машинном обучении для оценки неоднородности (или чистоты) данных в контексте задач классификации.

При построении разделений в деревьях решений или подобных моделях целью является выбор признака и порога разделения, которые минимально увеличат неоднородность (или максимально улучшат чистоту) исходной выборки. Этот процесс направлен на то, чтобы каждое разделение делало данные более однородными по отношению к целевой переменной. Таким образом, оптимальное разбиение должно приводить к подгруппам, в которых объекты более схожи или чётко разделены по своим классам или значениям целевой переменной. Этот подход позволяет строить деревья решений, которые эффективно разбивают данные на значимые сегменты для точного прогнозирования и классификации.

Формула, которую вы предоставили, выглядит как критерий информативности для построения разделений в дереве решений (или других моделях, использующих разбиение по признакам). Давайте разберем ее по частям:

1. \( Q(R) \) — это критерий, который мы хотим максимизировать при выборе разделения \( R \) на два подмножества \( R_l \) и \( R_r \).

2. \( H(R) \) — это мера неоднородности (или неопределенности) в текущем узле \( R \), которая может быть выражена через энтропию (или индекс Джини) в зависимости от контекста задачи. Обычно \( H(R) \) показывает, насколько разнородны данные в узле \( R \).

3. \( |R_l| \) и \( |R_r| \) — количество элементов в левом (\( R_l \)) и правом (\( R_r \)) подмножествах после разделения.

4. \( H(R_l) \) и \( H(R_r) \) — меры неоднородности (энтропия или индекс Джини) в левом и правом подмножествах соответственно после разделения.

Теперь давайте интерпретируем формулу:

- \( Q(R) \) вычисляется как разница между начальной неоднородностью \( H(R) \) и суммарным взвешенным уменьшением неоднородности после разделения на \( R_l \) и \( R_r \).

- Цель состоит в том, чтобы выбрать такое разделение \( R \), которое максимально уменьшает неоднородность в данных, тем самым улучшая качество разбиения и делая более однородными подмножества \( R_l \) и \( R_r \).

Эта формула является основой для многих алгоритмов построения деревьев решений, где целью является построение оптимального дерева, учитывающего неоднородность данных и максимизирующего информационный выигрыш при каждом разделении.

Конечные критерии остановки в обучении и разделении деревьев решений:

1. **Раннее прекращение (Early stopping)**: Алгоритм прекращает обучение, когда достигается определенное значение критерия, например, определенный процент правильно классифицированных примеров. Преимущество — сокращение времени обучения. Основной недостаток — потеря точности дерева.
    
2. **Ограничение глубины дерева (Limiting tree depth)**: Устанавливается максимальное количество разветвлений в ветвях, после чего обучение прекращается. Этот метод также снижает точность дерева.
    
3. **Установка минимального количества примеров в узле**: Запрещает алгоритму создавать узлы с менее заданного числа примеров, например, 5. Это предотвращает создание тривиальных разделений и, следовательно, незначительных правил.
    
4. **Остановка после исчезновения всей неоднородности в листьях**: Прекращение разделения, когда дальнейшее разделение не приводит к улучшению качества разделения.

Методы ансамблей (Ensemble Methods) в машинном обучении представляют собой подход, при котором несколько моделей обучаются для решения одной и той же задачи, а затем их прогнозы комбинируются для получения более качественного предсказания, чем у каждой отдельной модели.

The main idea of ensemble methods is that a sufficient number of weak algorithms together form a strong one.


Давайте начнем с ответа на следующий вопрос: на какие составляющие можно разложить ошибку любого прогноза, сделанного моделью машинного обучения?

Эти составляющие называются: смещение (bias), разброс (variance) и шум (noise).

1. **Смещение (Bias)**: Это ошибка, которая возникает из-за упрощенных предположений, сделанных моделью при обучении. Модель с высоким смещением склонна к недостаточно гибким или простым предсказаниям, которые могут быть недостаточно точными. Высокое смещение может возникнуть, если модель недообучена и не учитывает достаточно вариативности в данных.

2. **Разброс (Variance)**: Это ошибка, связанная с чувствительностью модели к изменениям в тренировочных данных. Модель с высоким разбросом может слишком точно подстраиваться под обучающие данные, что приводит к переобучению и плохой обобщающей способности на новых данных. Высокий разброс проявляется тогда, когда модель слишком сложна или обучена на недостаточно многочисленных данных.

3. **Шум (Noise)**: Это неизбежная ошибка, которая присутствует в данных и не может быть устранена моделью. Шум представляет собой нерегулярные колебания в данных, которые могут затруднять точное предсказание. Шум может включать в себя случайные вариации, ошибки измерений или другие неопределенности в данных.

Разложение ошибки на смещение, разброс и шум помогает понять, какие аспекты модели требуют улучшения. Например, высокое смещение требует более сложных моделей или большего количества признаков, чтобы улучшить адаптацию к данным. Высокий разброс может требовать регуляризации или уменьшения сложности модели для повышения ее обобщающей способности.

Компромисс между смещением и разбросом (Bias-Variance Tradeoff) представляет собой ключевой аспект в машинном обучении, который отражает баланс между двумя видами ошибок, возникающими при обучении модели.

1. **Смещение (Bias)**: Это ошибка модели, обусловленная неправильными или упрощенными предположениями о данных. Модель с высоким смещением может недооценивать сложность данных и делать систематически неверные прогнозы.

2. **Разброс (Variance)**: Это ошибка, связанная с чувствительностью модели к изменениям в тренировочных данных. Модель с высоким разбросом часто переобучается на обучающих данных и плохо обобщает новые данные.

**Основные идеи компромисса:**

- **Высокое смещение и низкий разброс**: Модель с высоким смещением и низким разбросом часто недообучена. Она делает упрощенные предсказания, которые могут быть несколько неточными, но при этом стабильными на различных выборках данных.

- **Низкое смещение и высокий разброс**: Модель с низким смещением и высоким разбросом может быть переобучена. Она может идеально подстраиваться под обучающие данные, но плохо обобщать на новые данные из-за чувствительности к случайным шумам в данных.

**Поиск оптимального решения**:

- Целью является построение модели, которая достигает наилучшего баланса между смещением и разбросом. Это может быть достигнуто путем выбора подходящей сложности модели, оптимизации параметров модели или использования методов регуляризации.

- Применение кросс-валидации, чтобы оценить, как модель обобщает данные, помогает контролировать баланс между смещением и разбросом.

**Практические примеры**: 

- В случае решающих деревьев глубина дерева является параметром, который может контролировать компромисс между смещением и разбросом.
  
- В линейной регрессии использование регуляризации (например, Lasso или Ridge) помогает снизить разброс модели, улучшая обобщающую способность.

Компромисс между смещением и разбросом является ключевым аспектом в создании эффективных моделей машинного обучения, которые хорошо обобщают данные и минимизируют ошибки прогнозирования.

Пример высокого смещения: если модель использует слишком простую формулу для предсказания, она может упускать сложные зависимости в данных, что приводит к недооценке предсказываемых значений.

Пример высокой дисперсии: если модель слишком сложна и слишком хорошо подстраивается под обучающие данные, она может начать интерпретировать случайные шумы или неточности в данных как значимые закономерности, что приведет к плохому качеству прогнозов на новых данных.