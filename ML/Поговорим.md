Linear space - A mathematical structure is a set of elements called vectors, for which the operations of addition with each other and multiplication by a number are defined - a scalar

In every linear space, there are several principal vectors called the basis. All other vectors can be expressed through these basis vectors. A basis is a minimal set of vectors such that:
1) None of them can be expressed as a linear combination of the others;
2) Every element of the space can be uniquely represented as a linear combination of a finite set of vectors from this set.

Определитель — числовая характеристика матрицы, которая в определенном смысле описывает сжатие или расширение пространства при её преобразовании.
Если мы связываем полученное значение с его геометрическим значением, учитывая, что мы преобразуем единичный квадрат с помощью матрицы:

- Отрицательный знак указывает на то, что наш квадрат будет иметь противоположную ориентацию после преобразования. Это означает, что оси координат в квадрате будут развернуты.

- Значение 2 указывает на то, что после преобразования площадь нашего квадрата будет в два раза больше, чем у исходного квадрата. Это говорит о том, что преобразование расширяет пространство в два раза по площади.

Feature matrix (матрица признаков) — это основная структура данных в задачах машинного обучения и анализа данных. Она представляет собой двумерную матрицу, где каждая строка соответствует отдельному наблюдению (или объекту), а каждый столбец представляет собой признак (feature), который описывает данный объект.

Айгенвектор — это вектор, который при преобразовании матрицей AAA сохраняет своё направление (или противоположное направление), но может изменяться его масштаб (длина).

### Шаги алгоритма PCA:

1. **Нахождение направления максимальной дисперсии:**
    
    Представьте, что у вас есть набор данных, который представляет собой облако точек в двумерном пространстве (например, набор из двух признаков). Цель PCA — найти направление (главную компоненту), вдоль которого данные имеют наибольшую дисперсию.
    
    На этом шаге PCA находит такое направление (главную компоненту), вдоль которого разброс точек (дисперсия) максимален. Это направление описывает основное направление изменчивости данных.
    
2. **Нахождение следующих главных компонент:**
    
    После того как найдена первая главная компонента, следующие компоненты ищутся среди оставшихся направлений, ортогональных (перпендикулярных) уже найденным компонентам. Каждая последующая компонента находится так, чтобы максимизировать оставшуюся дисперсию данных.
    
    На рисунке выше показаны две главные компоненты: первая (прямая линия) и вторая (перпендикулярная к первой). Вторая компонента ортогональна первой и описывает второе по величине направление изменчивости данных.
    
3. **Повторение для всех компонент:**
    
    Процесс повторяется до тех пор, пока не будет найдено определенное число главных компонент или пока не будет достигнута определенная часть дисперсии, которую мы хотим сохранить.

An orthogonal matrix is a matrix for which the inverse matrix is equal to its transpose.

Error function (функция ошибки) обычно относится к математической функции, которая используется для измерения ошибки или расхождения между фактическими и предсказанными значениями в задачах моделирования, статистики или машинного обучения.

1. **В контексте статистики и машинного обучения:** Error function часто представляет собой функцию, которая выражает расхождение между прогнозами модели и наблюдаемыми данными. Например, в задачах регрессии часто используется среднеквадратичная ошибка (Mean Squared Error, MSE) или средняя абсолютная ошибка (Mean Absolute Error, MAE) в качестве error function.
Основная цель - минимизация error function!

Предел функции означает, что значение функции f(x)приближается к числу L столь близко, насколько это возможно, когда x находится достаточно близко к a, но x не равно a.

Функция называется непрерывной (continuous), если она сохраняет свои значения без разрывов и прерываний. 
A function is said to be continuous on a set if it is continuous at every point of that set (the set can be a simple segment on the axis, for example).

Если функция изменяется почти так, как линейная функция вблизи определённой точки, то говорят, что она дифференцируема в этой точке.

Градиентный вектор — это вектор, который состоит из частных производных функции по каждой из её переменных. Он показывает направление наибольшего возрастания функции в данной точке.

The derivative in any direction canФункция потерь \( J(\theta) \):

\[ J(\theta) = (\theta - 5)^2 \]

Градиент функции потерь \( \nabla J(\theta) \):

\[ \nabla J(\theta) = \frac{\partial J(\theta)}{\partial \theta} = 2(\theta - 5) \]

Итерационное обновление параметра \( \theta \) с использованием градиентного спуска:

\[ \theta := \theta - \alpha \nabla J(\theta) \]

Здесь \( \alpha \) — скорость обучения.

Итерации градиентного спуска:

1. **Итерация 1:**
   \[ \theta := 0 - 0.2 \cdot (0 - 5) = 0 + 1 = 1 \]

2. **Итерация 2:**
   \[ \theta := 1 - 0.2 \cdot (1 - 5) = 1 + 0.8 = 1.8 \]

3. **Итерация 3:**
   \[ \theta := 1.8 - 0.2 \cdot (1.8 - 5) = 1.8 + 0.84 = 2.64 \]

4. **Итерация 4:**
   \[ \theta := 2.64 - 0.2 \cdot (2.64 - 5) = 2.64 + 0.672 = 3.312 \]

И так далее. Каждая итерация приближает \( \theta \) к значению \( 5 \), минимизируя функцию потерь \( J(\theta) \).
 be calculated through the dot product of the gradient with the directional vector of that direction.

Градиентный спуск : 

Функция потерь \( J(\theta) \):

\[ J(\theta) = (\theta - 5)^2 \]

Градиент функции потерь \( \nabla J(\theta) \):

\[ \nabla J(\theta) = \frac{\partial J(\theta)}{\partial \theta} = 2(\theta - 5) \]

Итерационное обновление параметра \( \theta \) с использованием градиентного спуска:

\[ \theta := \theta - \alpha \nabla J(\theta) \]

Здесь \( \alpha \) — скорость обучения.

Итерации градиентного спуска:

1. **Итерация 1:**
   \[ \theta := 0 - 0.2 \cdot (0 - 5) = 0 + 1 = 1 \]

2. **Итерация 2:**
   \[ \theta := 1 - 0.2 \cdot (1 - 5) = 1 + 0.8 = 1.8 \]

3. **Итерация 3:**
   \[ \theta := 1.8 - 0.2 \cdot (1.8 - 5) = 1.8 + 0.84 = 2.64 \]

4. **Итерация 4:**
   \[ \theta := 2.64 - 0.2 \cdot (2.64 - 5) = 2.64 + 0.672 = 3.312 \]

И так далее. Каждая итерация приближает \( \theta \) к значению \( 5 \), минимизируя функцию потерь \( J(\theta) \).

